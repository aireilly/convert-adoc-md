<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<article xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en" xml:id="cnf-numa-aware-scheduling">
<info>
<title>Scheduling NUMA-aware workloads</title>
<date>2025-09-08</date>
</info>

<simpara>Learn about NUMA-aware scheduling and how you can use it to deploy high performance workloads in an OpenShift cluster.</simpara>
<simpara>The NUMA Resources Operator allows you to schedule high-performance workloads in the same NUMA zone. It deploys a node resources exporting agent that reports on available cluster node NUMA resources, and a secondary scheduler that manages the workloads.</simpara>
<note>
<simpara>This is a WIP project playing with moving adoc to md.</simpara>
</note>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-rhel9:v4.18" <co xml:id="CO1-1"/></programlisting>
<calloutlist>
<callout arearefs="CO1-1">
<para>In a disconnected environment, make sure to configure the resolution of this image by either:</para>
<itemizedlist>
<listitem>
<simpara>Creating an <literal>ImageTagMirrorSet</literal> custom resource (CR).</simpara>
</listitem>
<listitem>
<simpara>Setting the URL to the disconnected registry.</simpara>
</listitem>
</itemizedlist>
</callout>
</calloutlist>
<table frame="all" rowsep="1" colsep="1">
<title>REPLACE_WITH_TABLE_TITLE</title>
<?dbhtml table-width="90%"?>
<?dbfo table-width="90%"?>
<?dblatex table-width="90%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="127.4998*"/>
<colspec colname="col_2" colwidth="255.0001*"/>
<thead>
<row>
<entry align="left" valign="top">Firefox</entry>
<entry align="left" valign="top">Web Browser</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Ruby</simpara></entry>
<entry align="left" valign="top"><simpara>Programming Language</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>TorqueBox</simpara></entry>
<entry align="left" valign="top"><simpara>Application Server</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="cnf-about-numa-aware-scheduling_numa-aware">
<title>About NUMA-aware scheduling</title>
<bridgehead xml:id="introduction-to-numa_numa-aware" renderas="sect2">Introduction to NUMA</bridgehead>
<simpara>Non-Uniform Memory Access (NUMA) is a compute platform architecture that allows different CPUs to access different regions of memory at different speeds. NUMA resource topology refers to the locations of CPUs, memory, and PCI devices relative to each other in the compute node. Colocated resources are said to be in the same <emphasis>NUMA zone</emphasis>. For high-performance applications, the cluster needs to process pod workloads in a single NUMA zone.</simpara>
<bridgehead xml:id="performance-considerations_numa-aware" renderas="sect2">Performance considerations</bridgehead>
<simpara>NUMA architecture allows a CPU with multiple memory controllers to use any available memory across CPU complexes, regardless of where the memory is located. This allows for increased flexibility at the expense of performance. A CPU processing a workload using memory that is outside its NUMA zone is slower than a workload processed in a single NUMA zone. Also, for I/O-constrained workloads, the network interface on a distant NUMA zone slows down how quickly information can reach the application. High-performance workloads, such as telecommunications workloads, cannot operate to specification under these conditions.</simpara>
<bridgehead xml:id="numa-aware-scheduling_numa-aware" renderas="sect2">NUMA-aware scheduling</bridgehead>
<simpara>NUMA-aware scheduling aligns the requested cluster compute resources (CPUs, memory, devices) in the same NUMA zone to process latency-sensitive or high-performance workloads efficiently. NUMA-aware scheduling also improves pod density per compute node for greater resource efficiency.</simpara>
<bridgehead xml:id="integration-with-node-tuning-operator_numa-aware" renderas="sect2">Integration with Node Tuning Operator</bridgehead>
<simpara>By integrating the Node Tuning Operator&#8217;s performance profile with NUMA-aware scheduling, you can further configure CPU affinity to optimize performance for latency-sensitive workloads.</simpara>
<bridgehead xml:id="default-scheduling-logic_numa-aware" renderas="sect2">Default scheduling logic</bridgehead>
<simpara>The default OpenShift pod scheduler scheduling logic considers the available resources of the entire compute node, not individual NUMA zones. If the most restrictive resource alignment is requested in the kubelet topology manager, error conditions can occur when admitting the pod to a node. Conversely, if the most restrictive resource alignment is not requested, the pod can be admitted to the node without proper resource alignment, leading to worse or unpredictable performance. For example, runaway pod creation with <literal>Topology Affinity Error</literal> statuses can occur when the pod scheduler makes suboptimal scheduling decisions for guaranteed pod workloads without knowing if the pod&#8217;s requested resources are available. Scheduling mismatch decisions can cause indefinite pod startup delays. Also, depending on the cluster state and resource allocation, poor pod scheduling decisions can cause extra load on the cluster because of failed startup attempts.</simpara>
<bridgehead xml:id="numa-aware-pod-scheduling-diagram_numa-aware" renderas="sect2">NUMA-aware pod scheduling diagram</bridgehead>
<simpara>The NUMA Resources Operator deploys a custom NUMA resources secondary scheduler and other resources to mitigate against the shortcomings of the default OpenShift pod scheduler. The following diagram provides a high-level overview of NUMA-aware pod scheduling.</simpara>
<figure>
<title>NUMA-aware scheduling overview</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/216_OpenShift_Topology-aware_Scheduling_0222.png"/>
</imageobject>
<textobject><phrase>Diagram of NUMA-aware scheduling that shows how the various components interact with each other in the cluster</phrase></textobject>
</mediaobject>
</figure>
<variablelist>
<varlistentry>
<term>NodeResourceTopology API</term>
<listitem>
<simpara>The <literal>NodeResourceTopology</literal> API describes the available NUMA zone resources in each compute node.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>NUMA-aware scheduler</term>
<listitem>
<simpara>The NUMA-aware secondary scheduler receives information about the available NUMA zones from the <literal>NodeResourceTopology</literal> API and schedules high-performance workloads on a node where it can be optimally processed.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Node topology exporter</term>
<listitem>
<simpara>The node topology exporter exposes the available NUMA zone resources for each compute node to the <literal>NodeResourceTopology</literal> API. The node topology exporter daemon tracks the resource allocation from the kubelet by using the <literal>PodResources</literal> API.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>PodResources API</term>
<listitem>
<simpara>The <literal>PodResources</literal> API is local to each node and exposes the resource topology and available resources to the kubelet.</simpara>
<note>
<simpara>The <literal>List</literal> endpoint of the <literal>PodResources</literal> API exposes exclusive CPUs allocated to a particular container. The API does not expose CPUs that belong to a shared pool.</simpara>
<simpara>The <literal>GetAllocatableResources</literal> endpoint exposes allocatable resources available on a node.</simpara>
</note>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="cnf-numa-resource-scheduling-strategies_numa-aware">
<title>NUMA resource scheduling strategies</title>
<simpara>When scheduling high-performance workloads, the secondary scheduler can employ different strategies to determine which NUMA node within a chosen worker node will handle the workload. The supported strategies in OpenShift include <literal>LeastAllocated</literal>, <literal>MostAllocated</literal>, and <literal>BalancedAllocation</literal>. Understanding these strategies helps optimize workload placement for performance and resource utilization.</simpara>
<simpara>When a high-performance workload is scheduled in a NUMA-aware cluster, the following steps occur:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The scheduler first selects a suitable worker node based on cluster-wide criteria. For example taints, labels, or resource availability.</simpara>
</listitem>
<listitem>
<simpara>After a worker node is selected, the scheduler evaluates its NUMA nodes and applies a scoring strategy to decide which NUMA node will handle the workload.</simpara>
</listitem>
<listitem>
<simpara>After a workload is scheduled, the selected NUMA nodeâ€™s resources are updated to reflect the allocation.</simpara>
</listitem>
</orderedlist>
<simpara>The default strategy applied is the <literal>LeastAllocated</literal> strategy. This assigns workloads to the NUMA node with the most available resources that is the least utilized NUMA node. The goal of this strategy is to spread workloads across NUMA nodes to reduce contention and avoid hotspots.</simpara>
<simpara>The following table summarizes the different strategies and their outcomes:</simpara>
<bridgehead xml:id="cnf-scoringstrategy-summary_numa-aware" renderas="sect2">Scoring strategy summary</bridgehead>
<table frame="all" rowsep="1" colsep="1">
<title>Scoring strategy summary</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="37.5*"/>
<colspec colname="col_3" colwidth="37.5*"/>
<thead>
<row>
<entry align="left" valign="top">Strategy</entry>
<entry align="left" valign="top">Description</entry>
<entry align="left" valign="top">Outcome</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>LeastAllocated</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Favors NUMA nodes with the most available resources.</simpara></entry>
<entry align="left" valign="top"><simpara>Spreads workloads to reduce contention and ensure headroom for high-priority tasks.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>MostAllocated</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Favors NUMA nodes with the least available resources.</simpara></entry>
<entry align="left" valign="top"><simpara>Consolidates workloads on fewer NUMA nodes, freeing others for energy efficiency.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>BalancedAllocation</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Favors NUMA nodes with balanced CPU and memory usage.</simpara></entry>
<entry align="left" valign="top"><simpara>Ensures even resource utilization, preventing skewed usage patterns.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<bridgehead xml:id="cnf-leastallocated-example_numa-aware" renderas="sect2">LeastAllocated strategy example</bridgehead>
<simpara>The <literal>LeastAllocated</literal> is the default strategy. This strategy assigns workloads to the NUMA node with the most available resources, minimizing resource contention and spreading workloads across NUMA nodes. This reduces hotspots and ensures sufficient headroom for high-priority tasks. Assume a worker node has two NUMA nodes, and the workload requires 4 vCPUs and 8 GB of memory:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Example initial NUMA nodes state</title>
<tgroup cols="6">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="13.3333*"/>
<colspec colname="col_3" colwidth="13.3333*"/>
<colspec colname="col_4" colwidth="13.3333*"/>
<colspec colname="col_5" colwidth="13.3333*"/>
<colspec colname="col_6" colwidth="13.3335*"/>
<thead>
<row>
<entry align="left" valign="top">NUMA node</entry>
<entry align="left" valign="top">Total CPUs</entry>
<entry align="left" valign="top">Used CPUs</entry>
<entry align="left" valign="top">Total memory (GB)</entry>
<entry align="left" valign="top">Used memory (GB)</entry>
<entry align="left" valign="top">Available resources</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>NUMA 1</simpara></entry>
<entry align="left" valign="top"><simpara>16</simpara></entry>
<entry align="left" valign="top"><simpara>12</simpara></entry>
<entry align="left" valign="top"><simpara>64</simpara></entry>
<entry align="left" valign="top"><simpara>56</simpara></entry>
<entry align="left" valign="top"><simpara>4 CPUs, 8 GB memory</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>NUMA 2</simpara></entry>
<entry align="left" valign="top"><simpara>16</simpara></entry>
<entry align="left" valign="top"><simpara>6</simpara></entry>
<entry align="left" valign="top"><simpara>64</simpara></entry>
<entry align="left" valign="top"><simpara>24</simpara></entry>
<entry align="left" valign="top"><simpara>10 CPUs, 40 GB memory</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>Because NUMA 2 has more available resources compared to NUMA 1, the workload is assigned to NUMA 2.</simpara>
<bridgehead xml:id="cnf-mostallocated-example_numa-aware" renderas="sect2">MostAllocated strategy example</bridgehead>
<simpara>The <literal>MostAllocated</literal> strategy consolidates workloads by assigning them to the NUMA node with the least available resources, which is the most utilized NUMA node. This approach helps free other NUMA nodes for energy efficiency or critical workloads requiring full isolation. This example uses the "Example initial NUMA nodes state" values listed in the <literal>LeastAllocated</literal> section.</simpara>
<simpara>The workload again requires 4 vCPUs and 8 GB memory. NUMA 1 has fewer available resources compared to NUMA 2, so the scheduler assigns the workload to NUMA 1, further utilizing its resources while leaving NUMA 2 idle or minimally loaded.</simpara>
<bridgehead xml:id="cnf-balanceallocated-example_numa-aware" renderas="sect2">BalancedAllocation strategy example</bridgehead>
<simpara>The <literal>BalancedAllocation</literal> strategy assigns workloads to the NUMA node with the most balanced resource utilization across CPU and memory. The goal is to prevent imbalanced usage, such as high CPU utilization with underutilized memory. Assume a worker node has the following NUMA node states:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Example NUMA nodes initial state for <literal>BalancedAllocation</literal></title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">NUMA node</entry>
<entry align="left" valign="top">CPU usage</entry>
<entry align="left" valign="top">Memory usage</entry>
<entry align="left" valign="top"><literal>BalancedAllocation</literal> score</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>NUMA 1</simpara></entry>
<entry align="left" valign="top"><simpara>60%</simpara></entry>
<entry align="left" valign="top"><simpara>55%</simpara></entry>
<entry align="left" valign="top"><simpara>High (more balanced)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>NUMA 2</simpara></entry>
<entry align="left" valign="top"><simpara>80%</simpara></entry>
<entry align="left" valign="top"><simpara>20%</simpara></entry>
<entry align="left" valign="top"><simpara>Low (less balanced)</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>NUMA 1 has a more balanced CPU and memory utilization compared to NUMA 2 and therefore, with the <literal>BalancedAllocation</literal> strategy in place, the workload is assigned to NUMA 1.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="../nodes/scheduling/secondary_scheduler/nodes-secondary-scheduler-configuring.xml#secondary-scheduler-configuring">Scheduling pods using a secondary scheduler</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../scalability_and_performance/cnf-numa-aware-scheduling.xml#cnf-changing-where-high-performance-workloads-run_numa-aware">Changing where high-performance workloads run</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="installing-the-numa-resources-operator_numa-aware">
<title>Installing the NUMA Resources Operator</title>
<simpara>NUMA Resources Operator deploys resources that allow you to schedule NUMA-aware workloads and deployments. You can install the NUMA Resources Operator using the OpenShift CLI or the web console.</simpara>
<section xml:id="cnf-installing-numa-resources-operator-cli_numa-aware">
<title>Installing the NUMA Resources Operator using the CLI</title>
<simpara>As a cluster administrator, you can install the Operator using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a namespace for the NUMA Resources Operator:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>nro-namespace.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-numaresources</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>Namespace</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f nro-namespace.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create the Operator group for the NUMA Resources Operator:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>nro-operatorgroup.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: numaresources-operator
  namespace: openshift-numaresources
spec:
  targetNamespaces:
  - openshift-numaresources</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>OperatorGroup</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f nro-operatorgroup.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create the subscription for the NUMA Resources Operator:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>nro-sub.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: numaresources-operator
  namespace: openshift-numaresources
spec:
  channel: "4.18"
  name: numaresources-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>Subscription</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f nro-sub.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that the installation succeeded by inspecting the CSV resource in the <literal>openshift-numaresources</literal> namespace. Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-numaresources</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                             DISPLAY                  VERSION   REPLACES   PHASE
numaresources-operator.v4.18.2   numaresources-operator   4.18.2               Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-installing-numa-resources-operator-console_numa-aware">
<title>Installing the NUMA Resources Operator using the web console</title>
<simpara>As a cluster administrator, you can install the NUMA Resources Operator using the web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a namespace for the NUMA Resources Operator:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>In the OpenShift web console, click <emphasis role="strong">Administration</emphasis> &#8594; <emphasis role="strong">Namespaces</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create Namespace</emphasis>, enter <literal>openshift-numaresources</literal> in the <emphasis role="strong">Name</emphasis> field, and then click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Install the NUMA Resources Operator:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>In the OpenShift web console, click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Choose <emphasis role="strong">numaresources-operator</emphasis> from the list of available Operators, and then click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Installed Namespaces</emphasis> field, select the <literal>openshift-numaresources</literal> namespace, and then click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Optional: Verify that the NUMA Resources Operator installed successfully:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Switch to the <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Ensure that <emphasis role="strong">NUMA Resources Operator</emphasis> is listed in the <literal>openshift-numaresources</literal> namespace with a <emphasis role="strong">Status</emphasis> of <emphasis role="strong">InstallSucceeded</emphasis>.</simpara>
<note>
<simpara>During installation an Operator might display a <emphasis role="strong">Failed</emphasis> status. If the installation later succeeds with an <emphasis role="strong">InstallSucceeded</emphasis> message, you can ignore the <emphasis role="strong">Failed</emphasis> message.</simpara>
</note>
<simpara>If the Operator does not appear as installed, to troubleshoot further:</simpara>
<itemizedlist>
<listitem>
<simpara>Go to the <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> page and inspect the <emphasis role="strong">Operator Subscriptions</emphasis> and <emphasis role="strong">Install Plans</emphasis> tabs for any failure or errors under <emphasis role="strong">Status</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Go to the <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> page and check the logs for pods in the <literal>default</literal> project.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="cnf-scheduling-numa-aware-workloads-overview_numa-aware">
<title>Scheduling NUMA-aware workloads</title>
<simpara>Clusters running latency-sensitive workloads typically feature performance profiles that help to minimize workload latency and optimize performance. The NUMA-aware scheduler deploys workloads based on available node NUMA resources and with respect to any performance profile settings applied to the node. The combination of NUMA-aware deployments, and the performance profile of the workload, ensures that workloads are scheduled in a way that maximizes performance.</simpara>
<simpara>For the NUMA Resources Operator to be fully operational, you must deploy the <literal>NUMAResourcesOperator</literal> custom resource and the NUMA-aware secondary pod scheduler.</simpara>
<section xml:id="cnf-creating-nrop-cr_numa-aware">
<title>Creating the NUMAResourcesOperator custom resource</title>
<simpara>When you have installed the NUMA Resources Operator, then create the <literal>NUMAResourcesOperator</literal> custom resource (CR) that instructs the NUMA Resources Operator to install all the cluster infrastructure needed to support the NUMA-aware scheduler, including daemon sets and APIs.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Install the NUMA Resources Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>NUMAResourcesOperator</literal> custom resource:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following minimal required YAML file example as <literal>nrop.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesOperator
metadata:
  name: numaresourcesoperator
spec:
  nodeGroups:
  - machineConfigPoolSelector:
      matchLabels:
        pools.operator.machineconfiguration.openshift.io/worker: "" <co xml:id="CO2-1"/></programlisting>
<calloutlist>
<callout arearefs="CO2-1">
<para>This must match the <literal>MachineConfigPool</literal> resource that you want to configure the NUMA Resources Operator on. For example, you might have created a <literal>MachineConfigPool</literal> resource named <literal>worker-cnf</literal> that designates a set of nodes expected to run telecommunications workloads. Each <literal>NodeGroup</literal> must match exactly one <literal>MachineConfigPool</literal>. Configurations where <literal>NodeGroup</literal> matches more than one <literal>MachineConfigPool</literal> are not supported.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>NUMAResourcesOperator</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f nrop.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Optional: To enable NUMA-aware scheduling for multiple machine config pools (MCPs), define a separate <literal>NodeGroup</literal> for each pool. For example, define three <literal>NodeGroups</literal> for <literal>worker-cnf</literal>, <literal>worker-ht</literal>, and <literal>worker-other</literal>, in the <literal>NUMAResourcesOperator</literal> CR as shown in the following example:</simpara>
<formalpara>
<title>Example YAML definition for a <literal>NUMAResourcesOperator</literal> CR with multiple <literal>NodeGroups</literal></title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesOperator
metadata:
  name: numaresourcesoperator
spec:
  logLevel: Normal
  nodeGroups:
    - machineConfigPoolSelector:
        matchLabels:
          machineconfiguration.openshift.io/role: worker-ht
    - machineConfigPoolSelector:
        matchLabels:
          machineconfiguration.openshift.io/role: worker-cnf
    - machineConfigPoolSelector:
        matchLabels:
          machineconfiguration.openshift.io/role: worker-other</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that the NUMA Resources Operator deployed successfully by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get numaresourcesoperators.nodetopology.openshift.io</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                    AGE
numaresourcesoperator   27s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>After a few minutes, run the following command to verify that the required resources deployed successfully:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-numaresources</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                    READY   STATUS    RESTARTS   AGE
pod/numaresources-controller-manager-7d9d84c58d-qk2mr   1/1     Running   0          12m
pod/numaresourcesoperator-worker-7d96r                  2/2     Running   0          97s
pod/numaresourcesoperator-worker-crsht                  2/2     Running   0          97s
pod/numaresourcesoperator-worker-jp9mw                  2/2     Running   0          97s</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-deploying-the-numa-aware-scheduler_numa-aware">
<title>Deploying the NUMA-aware secondary pod scheduler</title>
<simpara>After you install the NUMA Resources Operator, follow this procedure to deploy the NUMA-aware secondary pod scheduler.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>NUMAResourcesScheduler</literal> custom resource that deploys the NUMA-aware custom pod scheduler:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following minimal required YAML in the <literal>nro-scheduler.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-rhel9:v4.18" <co xml:id="CO3-1"/></programlisting>
<calloutlist>
<callout arearefs="CO3-1">
<para>In a disconnected environment, make sure to configure the resolution of this image by either:</para>
<itemizedlist>
<listitem>
<simpara>Creating an <literal>ImageTagMirrorSet</literal> custom resource (CR). For more information, see "Configuring image registry repository mirroring" in the "Additional resources" section.</simpara>
</listitem>
<listitem>
<simpara>Setting the URL to the disconnected registry.</simpara>
</listitem>
</itemizedlist>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>NUMAResourcesScheduler</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f nro-scheduler.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>After a few seconds, run the following command to confirm the successful deployment of the required resources:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-numaresources</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                    READY   STATUS    RESTARTS   AGE
pod/numaresources-controller-manager-7d9d84c58d-qk2mr   1/1     Running   0          12m
pod/numaresourcesoperator-worker-7d96r                  2/2     Running   0          97s
pod/numaresourcesoperator-worker-crsht                  2/2     Running   0          97s
pod/numaresourcesoperator-worker-jp9mw                  2/2     Running   0          97s
pod/secondary-scheduler-847cb74f84-9whlm                1/1     Running   0          10m

NAME                                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                     AGE
daemonset.apps/numaresourcesoperator-worker   3         3         3       3            3           node-role.kubernetes.io/worker=   98s

NAME                                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/numaresources-controller-manager   1/1     1            1           12m
deployment.apps/secondary-scheduler                1/1     1            1           10m

NAME                                                          DESIRED   CURRENT   READY   AGE
replicaset.apps/numaresources-controller-manager-7d9d84c58d   1         1         1       12m
replicaset.apps/secondary-scheduler-847cb74f84                1         1         1       10m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-configuring-single-numa-policy_numa-aware">
<title>Configuring a single NUMA node policy</title>
<simpara>The NUMA Resources Operator requires a single NUMA node policy to be configured on the cluster. This can be achieved in two ways: by creating and applying a performance profile, or by configuring a KubeletConfig.</simpara>
<note>
<simpara>The preferred way to configure a single NUMA node policy is to apply a performance profile. You can use the Performance Profile Creator (PPC) tool to create the performance profile. If a performance profile is created on the cluster, it automatically creates other tuning components like <literal>KubeletConfig</literal> and the <literal>tuned</literal> profile.</simpara>
</note>
<simpara>For more information about creating a performance profile, see "About the Performance Profile Creator" in the "Additional resources" section.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="../disconnected/updating/disconnected-update.xml#images-configuration-registry-mirror-configuring_updating-disconnected-cluster">Configuring image registry repository mirroring</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../scalability_and_performance/cnf-tuning-low-latency-nodes-with-perf-profile.xml#cnf-about-the-profile-creator-tool_cnf-low-latency-perf-profile">About the Performance Profile Creator</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="cnf-sample-performance-policy_numa-aware">
<title>Sample performance profile</title>
<simpara>This example YAML shows a performance profile created by using the performance profile creator (PPC) tool:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: performance
spec:
  cpu:
    isolated: "3"
    reserved: 0-2
  machineConfigPoolSelector:
    pools.operator.machineconfiguration.openshift.io/worker: "" <co xml:id="CO4-1"/>
  nodeSelector:
    node-role.kubernetes.io/worker: ""
  numa:
    topologyPolicy: single-numa-node <co xml:id="CO4-2"/>
  realTimeKernel:
    enabled: true
  workloadHints:
    highPowerConsumption: true
    perPodPowerManagement: false
    realTime: true</programlisting>
<calloutlist>
<callout arearefs="CO4-1">
<para>This should match the <literal>MachineConfigPool</literal> that you want to configure the NUMA Resources Operator on. For example, you might have created a <literal>MachineConfigPool</literal> named <literal>worker-cnf</literal> that designates a set of nodes that run telecommunications workloads.</para>
</callout>
<callout arearefs="CO4-2">
<para>The <literal>topologyPolicy</literal> must be set to <literal>single-numa-node</literal>. Ensure that this is the case by setting the <literal>topology-manager-policy</literal> argument to <literal>single-numa-node</literal> when running the PPC tool.</para>
</callout>
</calloutlist>
</section>
<section xml:id="cnf-configuring-kubelet-config-nro_numa-aware">
<title>Creating a KubeletConfig CR</title>
<simpara>The recommended way to configure a single NUMA node policy is to apply a performance profile. Another way is by creating and applying a <literal>KubeletConfig</literal> custom resource (CR), as shown in the following procedure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>KubeletConfig</literal> custom resource (CR) that configures the pod admittance policy for the machine profile:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>nro-kubeletconfig.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: worker-tuning
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <co xml:id="CO5-1"/>
  kubeletConfig:
    cpuManagerPolicy: "static" <co xml:id="CO5-2"/>
    cpuManagerReconcilePeriod: "5s"
    reservedSystemCPUs: "0,1" <co xml:id="CO5-3"/>
    memoryManagerPolicy: "Static" <co xml:id="CO5-4"/>
    evictionHard:
      memory.available: "100Mi"
    kubeReserved:
      memory: "512Mi"
    reservedMemory:
      - numaNode: 0
        limits:
          memory: "1124Mi"
    systemReserved:
      memory: "512Mi"
    topologyManagerPolicy: "single-numa-node" <co xml:id="CO5-5"/></programlisting>
<calloutlist>
<callout arearefs="CO5-1">
<para>Adjust this label to match the <literal>machineConfigPoolSelector</literal> in the <literal>NUMAResourcesOperator</literal> CR.</para>
</callout>
<callout arearefs="CO5-2">
<para>For <literal>cpuManagerPolicy</literal>, <literal>static</literal> must use a lowercase <literal>s</literal>.</para>
</callout>
<callout arearefs="CO5-3">
<para>Adjust this based on the CPU on your nodes.</para>
</callout>
<callout arearefs="CO5-4">
<para>For <literal>memoryManagerPolicy</literal>, <literal>Static</literal> must use an uppercase <literal>S</literal>.</para>
</callout>
<callout arearefs="CO5-5">
<para><literal>topologyManagerPolicy</literal> must be set to <literal>single-numa-node</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>KubeletConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f nro-kubeletconfig.yaml</programlisting>
<note>
<simpara>Applying performance profile or <literal>KubeletConfig</literal> automatically triggers rebooting of the nodes. If no reboot is triggered, you can troubleshoot the issue by looking at the labels in <literal>KubeletConfig</literal> that address the node group.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-scheduling-numa-aware-workloads_numa-aware">
<title>Scheduling workloads with the NUMA-aware scheduler</title>
<simpara>Now that <literal>topo-aware-scheduler</literal> is installed, the <literal>NUMAResourcesOperator</literal> and <literal>NUMAResourcesScheduler</literal> CRs are applied and your cluster has a matching performance profile or <literal>kubeletconfig</literal>, you can schedule workloads with the NUMA-aware scheduler using deployment CRs that specify the minimum required resources to process the workload.</simpara>
<simpara>The following example deployment uses NUMA-aware scheduling for a sample workload.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the name of the NUMA-aware scheduler that is deployed in the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get numaresourcesschedulers.nodetopology.openshift.io numaresourcesscheduler -o json | jq '.status.schedulerName'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">"topo-aware-scheduler"</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a <literal>Deployment</literal> CR that uses scheduler named <literal>topo-aware-scheduler</literal>, for example:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>nro-deployment.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: apps/v1
kind: Deployment
metadata:
  name: numa-deployment-1
  namespace: openshift-numaresources
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      schedulerName: topo-aware-scheduler <co xml:id="CO6-1"/>
      containers:
      - name: ctnr
        image: quay.io/openshifttest/hello-openshift:openshift
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: "100Mi"
            cpu: "10"
          requests:
            memory: "100Mi"
            cpu: "10"
      - name: ctnr2
        image: registry.access.redhat.com/rhel:latest
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "-c"]
        args: [ "while true; do sleep 1h; done;" ]
        resources:
          limits:
            memory: "100Mi"
            cpu: "8"
          requests:
            memory: "100Mi"
            cpu: "8"</programlisting>
<calloutlist>
<callout arearefs="CO6-1">
<para><literal>schedulerName</literal> must match the name of the NUMA-aware scheduler that is deployed in your cluster, for example <literal>topo-aware-scheduler</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>Deployment</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f nro-deployment.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that the deployment was successful:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-numaresources</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                READY   STATUS    RESTARTS   AGE
numa-deployment-1-6c4f5bdb84-wgn6g                  2/2     Running   0          5m2s
numaresources-controller-manager-7d9d84c58d-4v65j   1/1     Running   0          18m
numaresourcesoperator-worker-7d96r                  2/2     Running   4          43m
numaresourcesoperator-worker-crsht                  2/2     Running   2          43m
numaresourcesoperator-worker-jp9mw                  2/2     Running   2          43m
secondary-scheduler-847cb74f84-fpncj                1/1     Running   0          18m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>topo-aware-scheduler</literal> is scheduling the deployed pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe pod numa-deployment-1-6c4f5bdb84-wgn6g -n openshift-numaresources</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Events:
  Type    Reason          Age    From                  Message
  ----    ------          ----   ----                  -------
  Normal  Scheduled       4m45s  topo-aware-scheduler  Successfully assigned openshift-numaresources/numa-deployment-1-6c4f5bdb84-wgn6g to worker-1</programlisting>
</para>
</formalpara>
<note>
<simpara>Deployments that request more resources than is available for scheduling will fail with a <literal>MinimumReplicasUnavailable</literal> error. The deployment succeeds when the required resources become available. Pods remain in the <literal>Pending</literal> state until the required resources are available.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify that the expected allocated resources are listed for the node.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Identify the node that is running the deployment pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-numaresources -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                 READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
numa-deployment-1-6c4f5bdb84-wgn6g   0/2     Running   0          82m   10.128.2.50   worker-1   &lt;none&gt;  &lt;none&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Run the following command with the name of that node that is running the deployment pod.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe noderesourcetopologies.topology.node.k8s.io worker-1</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...

Zones:
  Costs:
    Name:   node-0
    Value:  10
    Name:   node-1
    Value:  21
  Name:     node-0
  Resources:
    Allocatable:  39
    Available:    21 <co xml:id="CO7-1"/>
    Capacity:     40
    Name:         cpu
    Allocatable:  6442450944
    Available:    6442450944
    Capacity:     6442450944
    Name:         hugepages-1Gi
    Allocatable:  134217728
    Available:    134217728
    Capacity:     134217728
    Name:         hugepages-2Mi
    Allocatable:  262415904768
    Available:    262206189568
    Capacity:     270146007040
    Name:         memory
  Type:           Node</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO7-1">
<para>The <literal>Available</literal> capacity is reduced because of the resources that have been allocated to the guaranteed pod.</para>
<simpara>Resources consumed by guaranteed pods are subtracted from the available node resources listed under <literal>noderesourcetopologies.topology.node.k8s.io</literal>.</simpara>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Resource allocations for pods with a <literal>Best-effort</literal> or <literal>Burstable</literal> quality of service (<literal>qosClass</literal>) are not reflected in the NUMA node resources under <literal>noderesourcetopologies.topology.node.k8s.io</literal>. If a pod&#8217;s consumed resources are not reflected in the node resource calculation, verify that the pod has <literal>qosClass</literal> of <literal>Guaranteed</literal> and the CPU request is an integer value, not a decimal value. You can verify the that the pod has a  <literal>qosClass</literal> of <literal>Guaranteed</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod numa-deployment-1-6c4f5bdb84-wgn6g -n openshift-numaresources -o jsonpath="{ .status.qosClass }"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Guaranteed</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="cnf-configuring-node-groups-for-the-numaresourcesoperator_numa-aware">
<title>Optional: Configuring polling operations for NUMA resources updates</title>
<simpara>The daemons controlled by the NUMA Resources Operator in their <literal>nodeGroup</literal> poll resources to retrieve updates about available NUMA resources. You can fine-tune polling operations for these daemons by configuring the <literal>spec.nodeGroups</literal> specification in the <literal>NUMAResourcesOperator</literal> custom resource (CR). This provides advanced control of polling operations. Configure these specifications to improve scheduling behavior and troubleshoot suboptimal scheduling decisions.</simpara>
<simpara>The configuration options are the following:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>infoRefreshMode</literal>: Determines the trigger condition for polling the kubelet. The NUMA Resources Operator reports the resulting information to the API server.</simpara>
</listitem>
<listitem>
<simpara><literal>infoRefreshPeriod</literal>: Determines the duration between polling updates.</simpara>
</listitem>
<listitem>
<simpara><literal>podsFingerprinting</literal>: Determines if point-in-time information for the current set of pods running on a node is exposed in polling updates.</simpara>
<note>
<simpara>The default value for <literal>podsFingerprinting</literal> is <literal>EnabledExclusiveResources</literal>. To optimize scheduler performance, set <literal>podsFingerprinting</literal> to either <literal>EnabledExclusiveResources</literal> or <literal>Enabled</literal>. Additionally, configure the <literal>cacheResyncPeriod</literal> in the <literal>NUMAResourcesScheduler</literal> custom resource (CR) to a value greater than 0. The <literal>cacheResyncPeriod</literal> specification helps to report more exact resource availability by monitoring pending resources on nodes.</simpara>
</note>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Install the NUMA Resources Operator.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Configure the <literal>spec.nodeGroups</literal> specification in your <literal>NUMAResourcesOperator</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesOperator
metadata:
  name: numaresourcesoperator
spec:
  nodeGroups:
  - config:
      infoRefreshMode: Periodic <co xml:id="CO8-1"/>
      infoRefreshPeriod: 10s <co xml:id="CO8-2"/>
      podsFingerprinting: Enabled <co xml:id="CO8-3"/>
    name: worker</programlisting>
<calloutlist>
<callout arearefs="CO8-1">
<para>Valid values are <literal>Periodic</literal>, <literal>Events</literal>, <literal>PeriodicAndEvents</literal>. Use <literal>Periodic</literal> to poll the kubelet at intervals that you define in <literal>infoRefreshPeriod</literal>. Use <literal>Events</literal> to poll the kubelet at every pod lifecycle event. Use <literal>PeriodicAndEvents</literal> to enable both methods.</para>
</callout>
<callout arearefs="CO8-2">
<para>Define the polling interval for <literal>Periodic</literal> or <literal>PeriodicAndEvents</literal> refresh modes. The field is ignored if the refresh mode is <literal>Events</literal>.</para>
</callout>
<callout arearefs="CO8-3">
<para>Valid values are <literal>Enabled</literal>, <literal>Disabled</literal>, and <literal>EnabledExclusiveResources</literal>. Setting to <literal>Enabled</literal> or <literal>EnabledExclusiveResources</literal> is a requirement for the <literal>cacheResyncPeriod</literal> specification in the <literal>NUMAResourcesScheduler</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>After you deploy the NUMA Resources Operator, verify that the node group configurations were applied by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get numaresop numaresourcesoperator -o json | jq '.status'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">      ...

        "config": {
        "infoRefreshMode": "Periodic",
        "infoRefreshPeriod": "10s",
        "podsFingerprinting": "Enabled"
      },
      "name": "worker"

      ...</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-troubleshooting-numa-aware-workloads_numa-aware">
<title>Troubleshooting NUMA-aware scheduling</title>
<simpara>To troubleshoot common problems with NUMA-aware pod scheduling, perform the following steps.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with cluster-admin privileges.</simpara>
</listitem>
<listitem>
<simpara>Install the NUMA Resources Operator and deploy the NUMA-aware secondary scheduler.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Verify that the <literal>noderesourcetopologies</literal> CRD is deployed in the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get crd | grep noderesourcetopologies</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                              CREATED AT
noderesourcetopologies.topology.node.k8s.io                       2022-01-18T08:28:06Z</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the NUMA-aware scheduler name matches the name specified in your NUMA-aware workloads by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get numaresourcesschedulers.nodetopology.openshift.io numaresourcesscheduler -o json | jq '.status.schedulerName'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">topo-aware-scheduler</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that NUMA-aware schedulable nodes have the <literal>noderesourcetopologies</literal> CR applied to them. Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get noderesourcetopologies.topology.node.k8s.io</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                    AGE
compute-0.example.com   17h
compute-1.example.com   17h</programlisting>
</para>
</formalpara>
<note>
<simpara>The number of nodes should equal the number of worker nodes that are configured by the machine config pool (<literal>mcp</literal>) worker definition.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the NUMA zone granularity for all schedulable nodes by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get noderesourcetopologies.topology.node.k8s.io -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
items:
- apiVersion: topology.node.k8s.io/v1
  kind: NodeResourceTopology
  metadata:
    annotations:
      k8stopoawareschedwg/rte-update: periodic
    creationTimestamp: "2022-06-16T08:55:38Z"
    generation: 63760
    name: worker-0
    resourceVersion: "8450223"
    uid: 8b77be46-08c0-4074-927b-d49361471590
  topologyPolicies:
  - SingleNUMANodeContainerLevel
  zones:
  - costs:
    - name: node-0
      value: 10
    - name: node-1
      value: 21
    name: node-0
    resources:
    - allocatable: "38"
      available: "38"
      capacity: "40"
      name: cpu
    - allocatable: "134217728"
      available: "134217728"
      capacity: "134217728"
      name: hugepages-2Mi
    - allocatable: "262352048128"
      available: "262352048128"
      capacity: "270107316224"
      name: memory
    - allocatable: "6442450944"
      available: "6442450944"
      capacity: "6442450944"
      name: hugepages-1Gi
    type: Node
  - costs:
    - name: node-0
      value: 21
    - name: node-1
      value: 10
    name: node-1
    resources:
    - allocatable: "268435456"
      available: "268435456"
      capacity: "268435456"
      name: hugepages-2Mi
    - allocatable: "269231067136"
      available: "269231067136"
      capacity: "270573244416"
      name: memory
    - allocatable: "40"
      available: "40"
      capacity: "40"
      name: cpu
    - allocatable: "1073741824"
      available: "1073741824"
      capacity: "1073741824"
      name: hugepages-1Gi
    type: Node
- apiVersion: topology.node.k8s.io/v1
  kind: NodeResourceTopology
  metadata:
    annotations:
      k8stopoawareschedwg/rte-update: periodic
    creationTimestamp: "2022-06-16T08:55:37Z"
    generation: 62061
    name: worker-1
    resourceVersion: "8450129"
    uid: e8659390-6f8d-4e67-9a51-1ea34bba1cc3
  topologyPolicies:
  - SingleNUMANodeContainerLevel
  zones: <co xml:id="CO9-1"/>
  - costs:
    - name: node-0
      value: 10
    - name: node-1
      value: 21
    name: node-0
    resources: <co xml:id="CO9-2"/>
    - allocatable: "38"
      available: "38"
      capacity: "40"
      name: cpu
    - allocatable: "6442450944"
      available: "6442450944"
      capacity: "6442450944"
      name: hugepages-1Gi
    - allocatable: "134217728"
      available: "134217728"
      capacity: "134217728"
      name: hugepages-2Mi
    - allocatable: "262391033856"
      available: "262391033856"
      capacity: "270146301952"
      name: memory
    type: Node
  - costs:
    - name: node-0
      value: 21
    - name: node-1
      value: 10
    name: node-1
    resources:
    - allocatable: "40"
      available: "40"
      capacity: "40"
      name: cpu
    - allocatable: "1073741824"
      available: "1073741824"
      capacity: "1073741824"
      name: hugepages-1Gi
    - allocatable: "268435456"
      available: "268435456"
      capacity: "268435456"
      name: hugepages-2Mi
    - allocatable: "269192085504"
      available: "269192085504"
      capacity: "270534262784"
      name: memory
    type: Node
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO9-1">
<para>Each stanza under <literal>zones</literal> describes the resources for a single NUMA zone.</para>
</callout>
<callout arearefs="CO9-2">
<para><literal>resources</literal> describes the current state of the NUMA zone resources. Check that resources listed under <literal>items.zones.resources.available</literal> correspond to the exclusive NUMA zone resources allocated to each guaranteed pod.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<section xml:id="cnf-reporting-more-exact-resource-availability_numa-aware">
<title>Reporting more exact resource availability</title>
<simpara>Enable the <literal>cacheResyncPeriod</literal> specification to help the NUMA Resources Operator report more exact resource availability by monitoring pending resources on nodes and synchronizing this information in the scheduler cache at a defined interval. This also helps to minimize Topology Affinity Error errors because of sub-optimal scheduling decisions. The lower the interval, the greater the network load. The <literal>cacheResyncPeriod</literal> specification is disabled by default.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the currently running <literal>NUMAResourcesScheduler</literal> resource:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the active <literal>NUMAResourcesScheduler</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get NUMAResourcesScheduler</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                     AGE
numaresourcesscheduler   92m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete the secondary scheduler resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete NUMAResourcesScheduler numaresourcesscheduler</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">numaresourcesscheduler.nodetopology.openshift.io "numaresourcesscheduler" deleted</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Save the following YAML in the file <literal>nro-scheduler-cacheresync.yaml</literal>. This example changes the log level to <literal>Debug</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-container-rhel8:v4.18"
  cacheResyncPeriod: "5s" <co xml:id="CO10-1"/></programlisting>
<calloutlist>
<callout arearefs="CO10-1">
<para>Enter an interval value in seconds for synchronization of the scheduler cache. A value of <literal>5s</literal> is typical for most implementations.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the updated <literal>NUMAResourcesScheduler</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f nro-scheduler-cacheresync.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">numaresourcesscheduler.nodetopology.openshift.io/numaresourcesscheduler created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification steps</title>
<listitem>
<simpara>Check that the NUMA-aware scheduler was successfully deployed:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Run the following command to check that the CRD is created successfully:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get crd | grep numaresourcesschedulers</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                              CREATED AT
numaresourcesschedulers.nodetopology.openshift.io                 2022-02-25T11:57:03Z</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the new custom scheduler is available by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get numaresourcesschedulers.nodetopology.openshift.io</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                     AGE
numaresourcesscheduler   3h26m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Check that the logs for the scheduler show the increased log level:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the list of pods running in the <literal>openshift-numaresources</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-numaresources</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                               READY   STATUS    RESTARTS   AGE
numaresources-controller-manager-d87d79587-76mrm   1/1     Running   0          46h
numaresourcesoperator-worker-5wm2k                 2/2     Running   0          45h
numaresourcesoperator-worker-pb75c                 2/2     Running   0          45h
secondary-scheduler-7976c4d466-qm4sc               1/1     Running   0          21m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Get the logs for the secondary scheduler pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs secondary-scheduler-7976c4d466-qm4sc -n openshift-numaresources</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...
I0223 11:04:55.614788       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.Namespace total 11 items received
I0223 11:04:56.609114       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.ReplicationController total 10 items received
I0223 11:05:22.626818       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.StorageClass total 7 items received
I0223 11:05:31.610356       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.PodDisruptionBudget total 7 items received
I0223 11:05:31.713032       1 eventhandlers.go:186] "Add event for scheduled pod" pod="openshift-marketplace/certified-operators-thtvq"
I0223 11:05:53.461016       1 eventhandlers.go:244] "Delete event for scheduled pod" pod="openshift-marketplace/certified-operators-thtvq"</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-changing-where-high-performance-workloads-run_numa-aware">
<title>Changing where high-performance workloads run</title>
<simpara>The NUMA-aware secondary scheduler is responsible for scheduling high-performance workloads on a worker node and within a NUMA node where the workloads can be optimally processed. By default, the secondary scheduler assigns workloads to the NUMA node within the chosen worker node that has the most available resources.</simpara>
<simpara>If you want to change where the workloads run, you can add the <literal>scoringStrategy</literal> setting to the <literal>NUMAResourcesScheduler</literal> custom resource and set its value to either <literal>MostAllocated</literal>  or <literal>BalancedAllocation</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the currently running <literal>NUMAResourcesScheduler</literal> resource by using the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the active <literal>NUMAResourcesScheduler</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get NUMAResourcesScheduler</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                     AGE
numaresourcesscheduler   92m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete the secondary scheduler resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete NUMAResourcesScheduler numaresourcesscheduler</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">numaresourcesscheduler.nodetopology.openshift.io "numaresourcesscheduler" deleted</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Save the following YAML in the file <literal>nro-scheduler-mostallocated.yaml</literal>. This example changes the <literal>scoringStrategy</literal> to <literal>MostAllocated</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-container-rhel8:v{product-version}"
  scoringStrategy:
        type: "MostAllocated" <co xml:id="CO11-1"/></programlisting>
<calloutlist>
<callout arearefs="CO11-1">
<para>If the <literal>scoringStrategy</literal> configuration is omitted, the default of <literal>LeastAllocated</literal> applies.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the updated <literal>NUMAResourcesScheduler</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f nro-scheduler-mostallocated.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">numaresourcesscheduler.nodetopology.openshift.io/numaresourcesscheduler created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Check that the NUMA-aware scheduler was successfully deployed by using the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Run the following command to check that the custom resource definition (CRD) is created successfully:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get crd | grep numaresourcesschedulers</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                              CREATED AT
numaresourcesschedulers.nodetopology.openshift.io                 2022-02-25T11:57:03Z</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the new custom scheduler is available by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get numaresourcesschedulers.nodetopology.openshift.io</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                     AGE
numaresourcesscheduler   3h26m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that the <literal>ScoringStrategy</literal> has been applied correctly by running the following command to check the relevant <literal>ConfigMap</literal> resource for the scheduler:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-numaresources cm topo-aware-scheduler-config -o yaml | grep scoring -A 1</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">scoringStrategy:
  type: MostAllocated</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-checking-numa-aware-scheduler-logs_numa-aware">
<title>Checking the NUMA-aware scheduler logs</title>
<simpara>Troubleshoot problems with the NUMA-aware scheduler by reviewing the logs. If required, you can increase the scheduler log level by modifying the <literal>spec.logLevel</literal> field of the <literal>NUMAResourcesScheduler</literal> resource. Acceptable values are <literal>Normal</literal>, <literal>Debug</literal>, and <literal>Trace</literal>, with <literal>Trace</literal> being the most verbose option.</simpara>
<note>
<simpara>To change the log level of the secondary scheduler, delete the running scheduler resource and re-deploy it with the changed log level. The scheduler is unavailable for scheduling new workloads during this downtime.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the currently running <literal>NUMAResourcesScheduler</literal> resource:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the active <literal>NUMAResourcesScheduler</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get NUMAResourcesScheduler</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                     AGE
numaresourcesscheduler   90m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete the secondary scheduler resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete NUMAResourcesScheduler numaresourcesscheduler</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">numaresourcesscheduler.nodetopology.openshift.io "numaresourcesscheduler" deleted</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Save the following YAML in the file <literal>nro-scheduler-debug.yaml</literal>. This example changes the log level to <literal>Debug</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-container-rhel8:v4.18"
  logLevel: Debug</programlisting>
</listitem>
<listitem>
<simpara>Create the updated <literal>Debug</literal> logging <literal>NUMAResourcesScheduler</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f nro-scheduler-debug.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">numaresourcesscheduler.nodetopology.openshift.io/numaresourcesscheduler created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification steps</title>
<listitem>
<simpara>Check that the NUMA-aware scheduler was successfully deployed:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Run the following command to check that the CRD is created successfully:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get crd | grep numaresourcesschedulers</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                              CREATED AT
numaresourcesschedulers.nodetopology.openshift.io                 2022-02-25T11:57:03Z</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the new custom scheduler is available by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get numaresourcesschedulers.nodetopology.openshift.io</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                     AGE
numaresourcesscheduler   3h26m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Check that the logs for the scheduler shows the increased log level:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the list of pods running in the <literal>openshift-numaresources</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-numaresources</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                               READY   STATUS    RESTARTS   AGE
numaresources-controller-manager-d87d79587-76mrm   1/1     Running   0          46h
numaresourcesoperator-worker-5wm2k                 2/2     Running   0          45h
numaresourcesoperator-worker-pb75c                 2/2     Running   0          45h
secondary-scheduler-7976c4d466-qm4sc               1/1     Running   0          21m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Get the logs for the secondary scheduler pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs secondary-scheduler-7976c4d466-qm4sc -n openshift-numaresources</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...
I0223 11:04:55.614788       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.Namespace total 11 items received
I0223 11:04:56.609114       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.ReplicationController total 10 items received
I0223 11:05:22.626818       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.StorageClass total 7 items received
I0223 11:05:31.610356       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.PodDisruptionBudget total 7 items received
I0223 11:05:31.713032       1 eventhandlers.go:186] "Add event for scheduled pod" pod="openshift-marketplace/certified-operators-thtvq"
I0223 11:05:53.461016       1 eventhandlers.go:244] "Delete event for scheduled pod" pod="openshift-marketplace/certified-operators-thtvq"</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-troubleshooting-resource-topo-exporter_numa-aware">
<title>Troubleshooting the resource topology exporter</title>
<simpara>Troubleshoot <literal>noderesourcetopologies</literal> objects where unexpected results are occurring by inspecting the corresponding <literal>resource-topology-exporter</literal> logs.</simpara>
<note>
<simpara>It is recommended that NUMA resource topology exporter instances in the cluster are named for nodes they refer to. For example, a worker node with the name <literal>worker</literal> should have a corresponding <literal>noderesourcetopologies</literal> object called <literal>worker</literal>.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the daemonsets managed by the NUMA Resources Operator. Each daemonset has a corresponding <literal>nodeGroup</literal> in the <literal>NUMAResourcesOperator</literal> CR. Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get numaresourcesoperators.nodetopology.openshift.io numaresourcesoperator -o jsonpath="{.status.daemonsets[0]}"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{"name":"numaresourcesoperator-worker","namespace":"openshift-numaresources"}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Get the label for the daemonset of interest using the value for <literal>name</literal> from the previous step:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get ds -n openshift-numaresources numaresourcesoperator-worker -o jsonpath="{.spec.selector.matchLabels}"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{"name":"resource-topology"}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Get the pods using the <literal>resource-topology</literal> label by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-numaresources -l name=resource-topology -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                 READY   STATUS    RESTARTS   AGE    IP            NODE
numaresourcesoperator-worker-5wm2k   2/2     Running   0          2d1h   10.135.0.64   compute-0.example.com
numaresourcesoperator-worker-pb75c   2/2     Running   0          2d1h   10.132.2.33   compute-1.example.com</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Examine the logs of the <literal>resource-topology-exporter</literal> container running on the worker pod that corresponds to the node you are troubleshooting. Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n openshift-numaresources -c resource-topology-exporter numaresourcesoperator-worker-pb75c</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">I0221 13:38:18.334140       1 main.go:206] using sysinfo:
reservedCpus: 0,1
reservedMemory:
  "0": 1178599424
I0221 13:38:18.334370       1 main.go:67] === System information ===
I0221 13:38:18.334381       1 sysinfo.go:231] cpus: reserved "0-1"
I0221 13:38:18.334493       1 sysinfo.go:237] cpus: online "0-103"
I0221 13:38:18.546750       1 main.go:72]
cpus: allocatable "2-103"
hugepages-1Gi:
  numa cell 0 -&gt; 6
  numa cell 1 -&gt; 1
hugepages-2Mi:
  numa cell 0 -&gt; 64
  numa cell 1 -&gt; 128
memory:
  numa cell 0 -&gt; 45758Mi
  numa cell 1 -&gt; 48372Mi</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-troubleshooting-missing-rte-config-maps_numa-aware">
<title>Correcting a missing resource topology exporter config map</title>
<simpara>If you install the NUMA Resources Operator in a cluster with misconfigured cluster settings, in some circumstances, the Operator is shown as active but the logs of the resource topology exporter (RTE) daemon set pods show that the configuration for the RTE is missing, for example:</simpara>
<programlisting language="text" linenumbering="unnumbered">Info: couldn't find configuration in "/etc/resource-topology-exporter/config.yaml"</programlisting>
<simpara>This log message indicates that the <literal>kubeletconfig</literal> with the required configuration was not properly applied in the cluster, resulting in a missing RTE <literal>configmap</literal>. For example, the following cluster is missing a <literal>numaresourcesoperator-worker</literal> <literal>configmap</literal> custom resource (CR):</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmap</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           DATA   AGE
0e2a6bd3.openshift-kni.io      0      6d21h
kube-root-ca.crt               1      6d21h
openshift-service-ca.crt       1      6d21h
topo-aware-scheduler-config    1      6d18h</programlisting>
</para>
</formalpara>
<simpara>In a correctly configured cluster, <literal>oc get configmap</literal> also returns a <literal>numaresourcesoperator-worker</literal> <literal>configmap</literal> CR.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with cluster-admin privileges.</simpara>
</listitem>
<listitem>
<simpara>Install the NUMA Resources Operator and deploy the NUMA-aware secondary scheduler.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Compare the values for <literal>spec.machineConfigPoolSelector.matchLabels</literal> in <literal>kubeletconfig</literal> and
<literal>metadata.labels</literal> in the <literal>MachineConfigPool</literal> (<literal>mcp</literal>) worker CR using the following commands:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Check the <literal>kubeletconfig</literal> labels by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubeletconfig -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">machineConfigPoolSelector:
  matchLabels:
    cnf-worker-tuning: enabled</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the <literal>mcp</literal> labels by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp worker -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">labels:
  machineconfiguration.openshift.io/mco-built-in: ""
  pools.operator.machineconfiguration.openshift.io/worker: ""</programlisting>
</para>
</formalpara>
<simpara>The <literal>cnf-worker-tuning: enabled</literal> label is not present in the <literal>MachineConfigPool</literal> object.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Edit the <literal>MachineConfigPool</literal> CR to include the missing label, for example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit mcp worker -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">labels:
  machineconfiguration.openshift.io/mco-built-in: ""
  pools.operator.machineconfiguration.openshift.io/worker: ""
  cnf-worker-tuning: enabled</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Apply the label changes and wait for the cluster to apply the updated configuration. Run the following command:</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Check that the missing <literal>numaresourcesoperator-worker</literal> <literal>configmap</literal> CR is applied:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmap</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           DATA   AGE
0e2a6bd3.openshift-kni.io      0      6d21h
kube-root-ca.crt               1      6d21h
numaresourcesoperator-worker   1      5m
openshift-service-ca.crt       1      6d21h
topo-aware-scheduler-config    1      6d18h</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="cnf-about-collecting-nro-data_numa-aware">
<title>Collecting NUMA Resources Operator data</title>
<simpara>You can use the <literal>oc adm must-gather</literal> CLI command to collect information about your cluster, including features and objects associated with the NUMA Resources Operator.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To collect NUMA Resources Operator data with <literal>must-gather</literal>, you must specify the NUMA Resources Operator <literal>must-gather</literal> image.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather --image=registry.redhat.io/openshift4/numaresources-must-gather-rhel9:v4.18</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
</article>