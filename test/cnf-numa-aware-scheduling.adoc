:_mod-docs-content-type: ASSEMBLY
[id="cnf-numa-aware-scheduling"]
= Scheduling NUMA-aware workloads
:_mod-docs-content-type: SNIPPET
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
// n-1 and n+1 OCP versions relative to the current branch's {product-version} attr
:ocp-nminus1: 4.18
:ocp-nplus1: 4.20
// Operating system attributes
:op-system-first: Red{nbsp}Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red{nbsp}Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:op-system-version-9: 9
:op-system-ai: Red{nbsp}Hat Enterprise Linux AI
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red{nbsp}Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:hybrid-console-url: link:https://console.redhat.com[Red Hat Hybrid Cloud Console]
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oadp-version: 1.5.0
:oadp-version-1-3: 1.3.6
:oadp-version-1-4: 1.4.4
:oadp-version-1-5: 1.5.0
:oadp-bsl-api: backupstoragelocations.velero.io
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:product-mirror-registry: Mirror registry for Red Hat OpenShift
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-title: Red{nbsp}Hat Advanced Cluster Management
:rh-rhacm-first: Red{nbsp}Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.13
:osc: OpenShift sandboxed containers
:osc-operator: OpenShift sandboxed containers Operator
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:external-secrets-operator: External Secrets Operator for Red Hat OpenShift
:external-secrets-operator-short: External Secrets Operator
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
:cli-manager: CLI Manager Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.16
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-first: Migration Toolkit for Containers (MTC)
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.8
:mtc-legacy-image: 1.7
:mtv-first: Migration Toolkit for Virtualization (MTV)
:mtv-short: MTV
:mtv-full: Migration Toolkit for Virtualization
:mtv-version: 2.8
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red{nbsp}Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red{nbsp}Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.18
:pipelines-version-number: 1.18
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.19
:HCOVersion: 4.19.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
// openshift virtualization engine (ove)
:ove-first: Red{nbsp}Hat OpenShift Virtualization Engine
:ove: OpenShift Virtualization Engine
//distributed tracing
:DTProductName: Red Hat OpenShift Distributed Tracing Platform
:DTShortName: Distributed Tracing Platform
:DTProductVersion: 3.1
:JaegerName: Red Hat OpenShift Distributed Tracing Platform (Jaeger)
:JaegerOperator: Red Hat OpenShift Distributed Tracing Platform
:JaegerShortName: Distributed Tracing Platform (Jaeger)
:JaegerOperator: Red Hat OpenShift Distributed Tracing Platform
:JaegerVersion: 1.53.0
:OTELName: Red{nbsp}Hat build of OpenTelemetry
:OTELShortName: Red{nbsp}Hat build of OpenTelemetry
:OTELOperator: Red{nbsp}Hat build of OpenTelemetry Operator
:OTELVersion: 0.93.0
:TempoName: Red Hat OpenShift Distributed Tracing Platform
:TempoShortName: Distributed Tracing Platform
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.1
//telco
//lightspeed
:ols-official: Red{nbsp}Hat OpenShift Lightspeed
:ols: OpenShift Lightspeed
//logging
:logging: logging
:logging-uc: Logging
:for: for Red{nbsp}Hat OpenShift
:clo: Red{nbsp}Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//observability
:ObservabilityLongName: Red{nbsp}Hat OpenShift Observability
:ObservabilityShortName: Observability
// Cluster Monitoring Operator
:cmo-first: Cluster Monitoring Operator (CMO)
:cmo-full: Cluster Monitoring Operator
:cmo-short: CMO
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red{nbsp}Hat OpenShift Dedicated
:product-rosa: Red{nbsp}Hat OpenShift Service on AWS
:SMProductName: Red{nbsp}Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.6.8
:MaistraVersion: 2.6
:KialiProduct: Kiali Operator provided by Red Hat
:SMPlugin: OpenShift Service Mesh Console (OSSMC) plugin
:SMPluginShort: OSSMC plugin
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red{nbsp}Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red{nbsp}Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
:sno-okd: single-node OKD
:sno-caps-okd: Single-node OKD
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red{nbsp}Hat OpenShift Local
:openshift-dev-spaces-productname: Red{nbsp}Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical Volume Manager (LVM) Storage
:lvms: LVM Storage
//Version-agnostic OLM
:olm-first: Operator Lifecycle Manager (OLM)
:olm: OLM
//Initial version of OLM that shipped with OCP 4, aka "v0" and f/k/a "existing" during OLM v1's pre-4.18 TP phase
:olmv0: OLM (Classic)
:olmv0-caps: OLM (Classic)
:olmv0-first: Operator Lifecycle Manager (OLM) Classic
:olmv0-first-caps: Operator Lifecycle Manager (OLM) Classic
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, f/k/a "1.0"
:olmv1: OLM v1
:olmv1-first: Operator Lifecycle Manager (OLM) v1
//
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
// ODF
:odf-first: Red{nbsp}Hat OpenShift Data Foundation (ODF)
:odf-full: Red{nbsp}Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
// IBU
:lcao: Lifecycle Agent
// Cloud provider names
// Alibaba Cloud
:alibaba: Alibaba Cloud
// Amazon Web Services (AWS)
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
// Google Cloud Platform (GCP)
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
//IBM Cloud Object Storage (COS)
:ibm-cloud-object-storage: IBM Cloud Object Storage (COS)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
// Microsoft Azure
:azure-first: Microsoft Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci-first-no-rt: Oracle Cloud Infrastructure (OCI)
:oci: OCI
:oci-ccm-full: Oracle Cloud Controller Manager (CCM)
:oci-ccm: Oracle CCM
:oci-csi-full: Oracle Container Storage Interface (CSI)
:oci-csi: Oracle CSI
:ocid-first: Oracle(R) Cloud Identifier (OCID)
:ocid: OCID
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:oci-c3: Oracle(R) Compute Cloud@Customer
:oci-c3-no-rt: Oracle Compute Cloud@Customer
:oci-c3-short: Compute Cloud@Customer
:oci-pca: Oracle(R) Private Cloud Appliance
:oci-pca-no-rt: Oracle Private Cloud Appliance
:oci-pca-short: Private Cloud Appliance
// Red Hat OpenStack Platform (RHOSP)/OpenStack
:rh-openstack-first: Red{nbsp}Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:rhoso-first: Red{nbsp}Hat OpenStack Services on OpenShift (RHOSO)
:rhoso: RHOSO
// VMware vSphere
:vmw-first: VMware vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Token-based auth products
//AWS Security Token Service
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Microsoft Entra Workload ID (FKA Azure Active Directory Workload Identities)
:entra-first: Microsoft Entra Workload ID
:entra-short: Workload ID
//Google Cloud Platform Workload Identity
:gcp-wid-first: Google Cloud Platform Workload Identity
:gcp-wid-short: GCP Workload Identity
// Cluster API terminology
// Cluster CAPI Operator
:cluster-capi-operator: Cluster CAPI Operator
// Cluster API Provider Amazon Web Services (AWS)
:cap-aws-first: Cluster API Provider Amazon Web Services (AWS)
:cap-aws-short: Cluster API Provider AWS
// Cluster API Provider Google Cloud Platform (GCP)
:cap-gcp-first: Cluster API Provider Google Cloud Platform (GCP)
:cap-gcp-short: Cluster API Provider GCP
// Cluster API Provider IBM Cloud
:cap-ibm-first: Cluster API Provider IBM Cloud
:cap-ibm-short: Cluster API Provider IBM Cloud
// Cluster API Provider Kubevirt
:cap-kubevirt-first: Cluster API Provider Kubevirt
:cap-kubevirt-short: Cluster API Provider Kubevirt
// Cluster API Provider Microsoft Azure
:cap-azure-first: Cluster API Provider Microsoft Azure
:cap-azure-short: Cluster API Provider Azure
// Cluster API Provider Nutanix
:cap-nutanix-first: Cluster API Provider Nutanix
:cap-nutanix-short: Cluster API Provider Nutanix
// Cluster API Provider OpenStack
:cap-openstack-first: Cluster API Provider OpenStack
:cap-openstack-short: Cluster API Provider OpenStack
// Cluster API Provider Oracle Cloud Infrastructure (OCI)
:cap-oci-first: Cluster API Provider Oracle Cloud Infrastructure (OCI)
:cap-oci-short: Cluster API Provider OCI
// Cluster API Provider VMware vSphere
:cap-vsphere-first: Cluster API Provider VMware vSphere
:cap-vsphere-short: Cluster API Provider vSphere
// Cluster API Provider Metal3
:cap-bare-metal-first: Cluster API Provider Metal3
:cap-bare-metal-short: Cluster API Provider Metal3
// Hosted control planes related attributes
:hcp-capital: Hosted control planes
:hcp: hosted control planes
:mce: multicluster engine for Kubernetes Operator
:mce-short: multicluster engine Operator
//AI names; OpenShift AI can be used as the family name
:rhoai-full: Red{nbsp}Hat OpenShift AI
:rhoai: RHOAI
:rhoai-diy: Red{nbsp}Hat OpenShift AI Self-Managed
:rhoai-cloud: Red{nbsp}Hat OpenShift AI Cloud Service
:ai-first: artificial intelligence (AI)
//RHEL AI attribute listed with RHEL family
//zero trust workload identity manager
:zero-trust-full: Zero Trust Workload Identity Manager
:spiffe-full: Secure Production Identity Framework for Everyone (SPIFFE)
:svid-full: SPIFFE Verifiable Identity Document (SVID)
:spire-full: SPIFFE Runtime Environment
// Formerly on-cluster image layering
:image-mode-os-caps: Image mode for OpenShift
:image-mode-os-lower: image mode for OpenShift
// Formerly on-cluster layering
:image-mode-os-on-caps: On-cluster image mode
:image-mode-os-on-lower: on-cluster image mode
// Formerly out-of-cluster layering
:image-mode-os-out-caps: Out-of-cluster image mode
:image-mode-os-out-lower: out-of-cluster image mode
:context: numa-aware

toc::[]

Learn about NUMA-aware scheduling and how you can use it to deploy high performance workloads in an {product-title} cluster.

:FeatureName: NUMA-aware scheduling

The NUMA Resources Operator allows you to schedule high-performance workloads in the same NUMA zone. It deploys a node resources exporting agent that reports on available cluster node NUMA resources, and a secondary scheduler that manages the workloads.

[NOTE]
====
This is a WIP project playing with moving adoc to md.
====

[source,yaml,subs="attributes+"]
----
apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-rhel9:v{product-version}" <1>
----
<1> In a disconnected environment, make sure to configure the resolution of this image by either:
* Creating an `ImageTagMirrorSet` custom resource (CR).
* Setting the URL to the disconnected registry.

.REPLACE_WITH_TABLE_TITLE
[cols="1,2", width="90%", options="header"]
|===
|Firefox
|Web Browser

|Ruby
|Programming Language

|TorqueBox
|Application Server
|===

:leveloffset: +1

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_mod-docs-content-type: CONCEPT
[id="cnf-about-numa-aware-scheduling_{context}"]
= About NUMA-aware scheduling

[discrete]
[id="introduction-to-numa_{context}"]
== Introduction to NUMA

Non-Uniform Memory Access (NUMA) is a compute platform architecture that allows different CPUs to access different regions of memory at different speeds. NUMA resource topology refers to the locations of CPUs, memory, and PCI devices relative to each other in the compute node. Colocated resources are said to be in the same _NUMA zone_. For high-performance applications, the cluster needs to process pod workloads in a single NUMA zone.

[discrete]
[id="performance-considerations_{context}"]
== Performance considerations

NUMA architecture allows a CPU with multiple memory controllers to use any available memory across CPU complexes, regardless of where the memory is located. This allows for increased flexibility at the expense of performance. A CPU processing a workload using memory that is outside its NUMA zone is slower than a workload processed in a single NUMA zone. Also, for I/O-constrained workloads, the network interface on a distant NUMA zone slows down how quickly information can reach the application. High-performance workloads, such as telecommunications workloads, cannot operate to specification under these conditions.

[discrete]
[id="numa-aware-scheduling_{context}"]
== NUMA-aware scheduling

NUMA-aware scheduling aligns the requested cluster compute resources (CPUs, memory, devices) in the same NUMA zone to process latency-sensitive or high-performance workloads efficiently. NUMA-aware scheduling also improves pod density per compute node for greater resource efficiency.

[discrete]
[id="integration-with-node-tuning-operator_{context}"]
== Integration with Node Tuning Operator

By integrating the Node Tuning Operator's performance profile with NUMA-aware scheduling, you can further configure CPU affinity to optimize performance for latency-sensitive workloads.

[discrete]
[id="default-scheduling-logic_{context}"]
== Default scheduling logic

The default {product-title} pod scheduler scheduling logic considers the available resources of the entire compute node, not individual NUMA zones. If the most restrictive resource alignment is requested in the kubelet topology manager, error conditions can occur when admitting the pod to a node. Conversely, if the most restrictive resource alignment is not requested, the pod can be admitted to the node without proper resource alignment, leading to worse or unpredictable performance. For example, runaway pod creation with `Topology Affinity Error` statuses can occur when the pod scheduler makes suboptimal scheduling decisions for guaranteed pod workloads without knowing if the pod's requested resources are available. Scheduling mismatch decisions can cause indefinite pod startup delays. Also, depending on the cluster state and resource allocation, poor pod scheduling decisions can cause extra load on the cluster because of failed startup attempts.


[discrete]
[id="numa-aware-pod-scheduling-diagram_{context}"]
== NUMA-aware pod scheduling diagram

The NUMA Resources Operator deploys a custom NUMA resources secondary scheduler and other resources to mitigate against the shortcomings of the default {product-title} pod scheduler. The following diagram provides a high-level overview of NUMA-aware pod scheduling.

.NUMA-aware scheduling overview
image::216_OpenShift_Topology-aware_Scheduling_0222.png[Diagram of NUMA-aware scheduling that shows how the various components interact with each other in the cluster]

NodeResourceTopology API:: The `NodeResourceTopology` API describes the available NUMA zone resources in each compute node.
NUMA-aware scheduler:: The NUMA-aware secondary scheduler receives information about the available NUMA zones from the `NodeResourceTopology` API and schedules high-performance workloads on a node where it can be optimally processed.
Node topology exporter:: The node topology exporter exposes the available NUMA zone resources for each compute node to the `NodeResourceTopology` API. The node topology exporter daemon tracks the resource allocation from the kubelet by using the `PodResources` API.
PodResources API:: The `PodResources` API is local to each node and exposes the resource topology and available resources to the kubelet.
+
[NOTE]
====
The `List` endpoint of the `PodResources` API exposes exclusive CPUs allocated to a particular container. The API does not expose CPUs that belong to a shared pool.

The `GetAllocatableResources` endpoint exposes allocatable resources available on a node.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_mod-docs-content-type: CONCEPT
[id="cnf-numa-resource-scheduling-strategies_{context}"]
= NUMA resource scheduling strategies

When scheduling high-performance workloads, the secondary scheduler can employ different strategies to determine which NUMA node within a chosen worker node will handle the workload. The supported strategies in {product-title} include `LeastAllocated`, `MostAllocated`, and `BalancedAllocation`. Understanding these strategies helps optimize workload placement for performance and resource utilization.

When a high-performance workload is scheduled in a NUMA-aware cluster, the following steps occur:

.  The scheduler first selects a suitable worker node based on cluster-wide criteria. For example taints, labels, or resource availability.

. After a worker node is selected, the scheduler evaluates its NUMA nodes and applies a scoring strategy to decide which NUMA node will handle the workload.

. After a workload is scheduled, the selected NUMA node’s resources are updated to reflect the allocation.

The default strategy applied is the `LeastAllocated` strategy. This assigns workloads to the NUMA node with the most available resources that is the least utilized NUMA node. The goal of this strategy is to spread workloads across NUMA nodes to reduce contention and avoid hotspots.

The following table summarizes the different strategies and their outcomes:

[discrete]
[id="cnf-scoringstrategy-summary_{context}"]
== Scoring strategy summary

.Scoring strategy summary
[cols="2,3,3", options="header"]
|===
|Strategy |Description |Outcome
|`LeastAllocated` |Favors NUMA nodes with the most available resources. |Spreads workloads to reduce contention and ensure headroom for high-priority tasks.
|`MostAllocated` |Favors NUMA nodes with the least available resources. |Consolidates workloads on fewer NUMA nodes, freeing others for energy efficiency.
|`BalancedAllocation` |Favors NUMA nodes with balanced CPU and memory usage. |Ensures even resource utilization, preventing skewed usage patterns.
|===

[discrete]
[id="cnf-leastallocated-example_{context}"]
== LeastAllocated strategy example
The `LeastAllocated` is the default strategy. This strategy assigns workloads to the NUMA node with the most available resources, minimizing resource contention and spreading workloads across NUMA nodes. This reduces hotspots and ensures sufficient headroom for high-priority tasks. Assume a worker node has two NUMA nodes, and the workload requires 4 vCPUs and 8 GB of memory:

.Example initial NUMA nodes state
[cols="5,2,2,2,2,2", options="header"]
|===
|NUMA node |Total CPUs |Used CPUs |Total memory (GB) |Used memory (GB) |Available resources
|NUMA 1 |16 |12 |64 |56 |4 CPUs, 8 GB memory
|NUMA 2 |16 |6 |64 |24 |10 CPUs, 40 GB memory
|===

Because NUMA 2 has more available resources compared to NUMA 1, the workload is assigned to NUMA 2.

[discrete]
[id="cnf-mostallocated-example_{context}"]
== MostAllocated strategy example
The `MostAllocated` strategy consolidates workloads by assigning them to the NUMA node with the least available resources, which is the most utilized NUMA node. This approach helps free other NUMA nodes for energy efficiency or critical workloads requiring full isolation. This example uses the "Example initial NUMA nodes state" values listed in the `LeastAllocated` section.

The workload again requires 4 vCPUs and 8 GB memory. NUMA 1 has fewer available resources compared to NUMA 2, so the scheduler assigns the workload to NUMA 1, further utilizing its resources while leaving NUMA 2 idle or minimally loaded.

[discrete]
[id="cnf-balanceallocated-example_{context}"]
== BalancedAllocation strategy example
The `BalancedAllocation` strategy assigns workloads to the NUMA node with the most balanced resource utilization across CPU and memory. The goal is to prevent imbalanced usage, such as high CPU utilization with underutilized memory. Assume a worker node has the following NUMA node states:

.Example NUMA nodes initial state for `BalancedAllocation`
[cols="2,2,2,2",options="header"]
|===
|NUMA node |CPU usage |Memory usage |`BalancedAllocation` score
|NUMA 1 |60% |55% |High (more balanced)
|NUMA 2 |80% |20% |Low (less balanced)
|===

NUMA 1 has a more balanced CPU and memory utilization compared to NUMA 2 and therefore, with the `BalancedAllocation` strategy in place, the workload is assigned to NUMA 1.


:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../nodes/scheduling/secondary_scheduler/nodes-secondary-scheduler-configuring.adoc#secondary-scheduler-configuring[Scheduling pods using a secondary scheduler]

* xref:../scalability_and_performance/cnf-numa-aware-scheduling.adoc#cnf-changing-where-high-performance-workloads-run_numa-aware[Changing where high-performance workloads run]

[id="installing-the-numa-resources-operator_{context}"]
== Installing the NUMA Resources Operator

NUMA Resources Operator deploys resources that allow you to schedule NUMA-aware workloads and deployments. You can install the NUMA Resources Operator using the {product-title} CLI or the web console.

:leveloffset: +2

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_mod-docs-content-type: PROCEDURE
[id="cnf-installing-numa-resources-operator-cli_{context}"]
= Installing the NUMA Resources Operator using the CLI

As a cluster administrator, you can install the Operator using the CLI.

.Prerequisites

* Install the OpenShift CLI (`oc`).

* Log in as a user with `cluster-admin` privileges.

.Procedure

. Create a namespace for the NUMA Resources Operator:

.. Save the following YAML in the `nro-namespace.yaml` file:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-numaresources
----

.. Create the `Namespace` CR by running the following command:
+
[source,terminal]
----
$ oc create -f nro-namespace.yaml
----

. Create the Operator group for the NUMA Resources Operator:

.. Save the following YAML in the `nro-operatorgroup.yaml` file:
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: numaresources-operator
  namespace: openshift-numaresources
spec:
  targetNamespaces:
  - openshift-numaresources
----

.. Create the `OperatorGroup` CR by running the following command:
+
[source,terminal]
----
$ oc create -f nro-operatorgroup.yaml
----

. Create the subscription for the NUMA Resources Operator:

.. Save the following YAML in the `nro-sub.yaml` file:
+
[source,yaml,subs="attributes+"]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: numaresources-operator
  namespace: openshift-numaresources
spec:
  channel: "{product-version}"
  name: numaresources-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
----

.. Create the `Subscription` CR by running the following command:
+
[source,terminal]
----
$ oc create -f nro-sub.yaml
----

.Verification

. Verify that the installation succeeded by inspecting the CSV resource in the `openshift-numaresources` namespace. Run the following command:
+
[source,terminal]
----
$ oc get csv -n openshift-numaresources
----
+
.Example output

[source,terminal,subs="attributes+"]
----
NAME                             DISPLAY                  VERSION   REPLACES   PHASE
numaresources-operator.v{product-version}.2   numaresources-operator   {product-version}.2               Succeeded
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_mod-docs-content-type: PROCEDURE
[id="cnf-installing-numa-resources-operator-console_{context}"]
= Installing the NUMA Resources Operator using the web console

As a cluster administrator, you can install the NUMA Resources Operator using the web console.

.Procedure

. Create a namespace for the NUMA Resources Operator:

.. In the {product-title} web console, click *Administration* -> *Namespaces*.

.. Click *Create Namespace*, enter `openshift-numaresources` in the *Name* field, and then click *Create*.

. Install the NUMA Resources Operator:

.. In the {product-title} web console, click *Operators* -> *OperatorHub*.

.. Choose *numaresources-operator* from the list of available Operators, and then click *Install*.

.. In the *Installed Namespaces* field, select the `openshift-numaresources` namespace, and then click *Install*.

. Optional: Verify that the NUMA Resources Operator installed successfully:

.. Switch to the *Operators* -> *Installed Operators* page.

.. Ensure that *NUMA Resources Operator* is listed in the `openshift-numaresources` namespace with a *Status* of *InstallSucceeded*.
+
[NOTE]
====
During installation an Operator might display a *Failed* status. If the installation later succeeds with an *InstallSucceeded* message, you can ignore the *Failed* message.
====
+
If the Operator does not appear as installed, to troubleshoot further:
+
* Go to the *Operators* -> *Installed Operators* page and inspect the *Operator Subscriptions* and *Install Plans* tabs for any failure or errors under *Status*.
* Go to the *Workloads* -> *Pods* page and check the logs for pods in the `default` project.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc
:_mod-docs-content-type: CONCEPT
[id="cnf-scheduling-numa-aware-workloads-overview_{context}"]
= Scheduling NUMA-aware workloads

Clusters running latency-sensitive workloads typically feature performance profiles that help to minimize workload latency and optimize performance. The NUMA-aware scheduler deploys workloads based on available node NUMA resources and with respect to any performance profile settings applied to the node. The combination of NUMA-aware deployments, and the performance profile of the workload, ensures that workloads are scheduled in a way that maximizes performance.

For the NUMA Resources Operator to be fully operational, you must deploy the `NUMAResourcesOperator` custom resource and the NUMA-aware secondary pod scheduler.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_module-type: PROCEDURE
[id="cnf-creating-nrop-cr_{context}"]
= Creating the NUMAResourcesOperator custom resource

When you have installed the NUMA Resources Operator, then create the `NUMAResourcesOperator` custom resource (CR) that instructs the NUMA Resources Operator to install all the cluster infrastructure needed to support the NUMA-aware scheduler, including daemon sets and APIs.

.Prerequisites

* Install the OpenShift CLI (`oc`).
* Log in as a user with `cluster-admin` privileges.
* Install the NUMA Resources Operator.

.Procedure

. Create the `NUMAResourcesOperator` custom resource:

.. Save the following minimal required YAML file example as `nrop.yaml`:
+
[source,yaml]
----
apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesOperator
metadata:
  name: numaresourcesoperator
spec:
  nodeGroups:
  - machineConfigPoolSelector:
      matchLabels:
        pools.operator.machineconfiguration.openshift.io/worker: "" <1>
----
+
<1> This must match the `MachineConfigPool` resource that you want to configure the NUMA Resources Operator on. For example, you might have created a `MachineConfigPool` resource named `worker-cnf` that designates a set of nodes expected to run telecommunications workloads. Each `NodeGroup` must match exactly one `MachineConfigPool`. Configurations where `NodeGroup` matches more than one `MachineConfigPool` are not supported.

.. Create the `NUMAResourcesOperator` CR by running the following command:
+
[source,terminal]
----
$ oc create -f nrop.yaml
----

. Optional: To enable NUMA-aware scheduling for multiple machine config pools (MCPs), define a separate `NodeGroup` for each pool. For example, define three `NodeGroups` for `worker-cnf`, `worker-ht`, and `worker-other`, in the `NUMAResourcesOperator` CR as shown in the following example:
+
.Example YAML definition for a `NUMAResourcesOperator` CR with multiple `NodeGroups`
[source,yaml]
----
apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesOperator
metadata:
  name: numaresourcesoperator
spec:
  logLevel: Normal
  nodeGroups:
    - machineConfigPoolSelector:
        matchLabels:
          machineconfiguration.openshift.io/role: worker-ht
    - machineConfigPoolSelector:
        matchLabels:
          machineconfiguration.openshift.io/role: worker-cnf
    - machineConfigPoolSelector:
        matchLabels:
          machineconfiguration.openshift.io/role: worker-other
----

.Verification

. Verify that the NUMA Resources Operator deployed successfully by running the following command:
+
[source,terminal]
----
$ oc get numaresourcesoperators.nodetopology.openshift.io
----
+
.Example output
[source,terminal]
----
NAME                    AGE
numaresourcesoperator   27s
----

. After a few minutes, run the following command to verify that the required resources deployed successfully:
+
[source,terminal]
----
$ oc get all -n openshift-numaresources
----
+
.Example output
[source,terminal]
----
NAME                                                    READY   STATUS    RESTARTS   AGE
pod/numaresources-controller-manager-7d9d84c58d-qk2mr   1/1     Running   0          12m
pod/numaresourcesoperator-worker-7d96r                  2/2     Running   0          97s
pod/numaresourcesoperator-worker-crsht                  2/2     Running   0          97s
pod/numaresourcesoperator-worker-jp9mw                  2/2     Running   0          97s
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_module-type: PROCEDURE
[id="cnf-deploying-the-numa-aware-scheduler_{context}"]
= Deploying the NUMA-aware secondary pod scheduler

After you install the NUMA Resources Operator, follow this procedure to deploy the NUMA-aware secondary pod scheduler.

.Procedure
. Create the `NUMAResourcesScheduler` custom resource that deploys the NUMA-aware custom pod scheduler:

.. Save the following minimal required YAML in the `nro-scheduler.yaml` file:
+
[source,yaml,subs="attributes+"]
----
apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-rhel9:v{product-version}" <1>
----
+
<1> In a disconnected environment, make sure to configure the resolution of this image by either:

* Creating an `ImageTagMirrorSet` custom resource (CR). For more information, see "Configuring image registry repository mirroring" in the "Additional resources" section.

* Setting the URL to the disconnected registry.

.. Create the `NUMAResourcesScheduler` CR by running the following command:
+
[source,terminal]
----
$ oc create -f nro-scheduler.yaml
----

. After a few seconds, run the following command to confirm the successful deployment of the required resources:
+
[source,terminal]
----
$ oc get all -n openshift-numaresources
----
+
.Example output
[source,terminal]
----
NAME                                                    READY   STATUS    RESTARTS   AGE
pod/numaresources-controller-manager-7d9d84c58d-qk2mr   1/1     Running   0          12m
pod/numaresourcesoperator-worker-7d96r                  2/2     Running   0          97s
pod/numaresourcesoperator-worker-crsht                  2/2     Running   0          97s
pod/numaresourcesoperator-worker-jp9mw                  2/2     Running   0          97s
pod/secondary-scheduler-847cb74f84-9whlm                1/1     Running   0          10m

NAME                                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                     AGE
daemonset.apps/numaresourcesoperator-worker   3         3         3       3            3           node-role.kubernetes.io/worker=   98s

NAME                                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/numaresources-controller-manager   1/1     1            1           12m
deployment.apps/secondary-scheduler                1/1     1            1           10m

NAME                                                          DESIRED   CURRENT   READY   AGE
replicaset.apps/numaresources-controller-manager-7d9d84c58d   1         1         1       12m
replicaset.apps/secondary-scheduler-847cb74f84                1         1         1       10m
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_module-type: PROCEDURE
[id="cnf-configuring-single-numa-policy_{context}"]
= Configuring a single NUMA node policy

The NUMA Resources Operator requires a single NUMA node policy to be configured on the cluster. This can be achieved in two ways: by creating and applying a performance profile, or by configuring a KubeletConfig.

[NOTE]
====
The preferred way to configure a single NUMA node policy is to apply a performance profile. You can use the Performance Profile Creator (PPC) tool to create the performance profile. If a performance profile is created on the cluster, it automatically creates other tuning components like `KubeletConfig` and the `tuned` profile.
====

For more information about creating a performance profile, see "About the Performance Profile Creator" in the "Additional resources" section.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../disconnected/updating/disconnected-update.adoc#images-configuration-registry-mirror-configuring_updating-disconnected-cluster[Configuring image registry repository mirroring]

* xref:../scalability_and_performance/cnf-tuning-low-latency-nodes-with-perf-profile.adoc#cnf-about-the-profile-creator-tool_cnf-low-latency-perf-profile[About the Performance Profile Creator]

:leveloffset: +2

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_module-type: REFERENCE
[id="cnf-sample-performance-policy_{context}"]
= Sample performance profile

This example YAML shows a performance profile created by using the performance profile creator (PPC) tool:

[source,yaml]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: performance
spec:
  cpu:
    isolated: "3"
    reserved: 0-2
  machineConfigPoolSelector:
    pools.operator.machineconfiguration.openshift.io/worker: "" <1>
  nodeSelector:
    node-role.kubernetes.io/worker: ""
  numa:
    topologyPolicy: single-numa-node <2>
  realTimeKernel:
    enabled: true
  workloadHints:
    highPowerConsumption: true
    perPodPowerManagement: false
    realTime: true
----

<1> This should match the `MachineConfigPool` that you want to configure the NUMA Resources Operator on. For example, you might have created a `MachineConfigPool` named `worker-cnf` that designates a set of nodes that run telecommunications workloads.
<2> The `topologyPolicy` must be set to `single-numa-node`. Ensure that this is the case by setting the `topology-manager-policy` argument to `single-numa-node` when running the PPC tool.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_module-type: PROCEDURE
[id="cnf-configuring-kubelet-config-nro_{context}"]
= Creating a KubeletConfig CR

The recommended way to configure a single NUMA node policy is to apply a performance profile. Another way is by creating and applying a `KubeletConfig` custom resource (CR), as shown in the following procedure.

.Procedure

. Create the `KubeletConfig` custom resource (CR) that configures the pod admittance policy for the machine profile:

.. Save the following YAML in the `nro-kubeletconfig.yaml` file:
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: worker-tuning
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <1>
  kubeletConfig:
    cpuManagerPolicy: "static" <2>
    cpuManagerReconcilePeriod: "5s"
    reservedSystemCPUs: "0,1" <3>
    memoryManagerPolicy: "Static" <4>
    evictionHard:
      memory.available: "100Mi"
    kubeReserved:
      memory: "512Mi"
    reservedMemory:
      - numaNode: 0
        limits:
          memory: "1124Mi"
    systemReserved:
      memory: "512Mi"
    topologyManagerPolicy: "single-numa-node" <5>
----
<1> Adjust this label to match the `machineConfigPoolSelector` in the `NUMAResourcesOperator` CR.
<2> For `cpuManagerPolicy`, `static` must use a lowercase `s`.
<3> Adjust this based on the CPU on your nodes.
<4> For `memoryManagerPolicy`, `Static` must use an uppercase `S`.
<5> `topologyManagerPolicy` must be set to `single-numa-node`.

.. Create the `KubeletConfig` CR by running the following command:
+
[source,terminal]
----
$ oc create -f nro-kubeletconfig.yaml
----
+
[NOTE]
====
Applying performance profile or `KubeletConfig` automatically triggers rebooting of the nodes. If no reboot is triggered, you can troubleshoot the issue by looking at the labels in `KubeletConfig` that address the node group.
====

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_mod-docs-content-type: PROCEDURE
[id="cnf-scheduling-numa-aware-workloads_{context}"]
= Scheduling workloads with the NUMA-aware scheduler

Now that `topo-aware-scheduler` is installed, the `NUMAResourcesOperator` and `NUMAResourcesScheduler` CRs are applied and your cluster has a matching performance profile or `kubeletconfig`, you can schedule workloads with the NUMA-aware scheduler using deployment CRs that specify the minimum required resources to process the workload.

The following example deployment uses NUMA-aware scheduling for a sample workload.

.Prerequisites

* Install the OpenShift CLI (`oc`).

* Log in as a user with `cluster-admin` privileges.

.Procedure

. Get the name of the NUMA-aware scheduler that is deployed in the cluster by running the following command:
+
[source,terminal]
----
$ oc get numaresourcesschedulers.nodetopology.openshift.io numaresourcesscheduler -o json | jq '.status.schedulerName'
----
+
.Example output
[source,terminal]
----
"topo-aware-scheduler"
----

. Create a `Deployment` CR that uses scheduler named `topo-aware-scheduler`, for example:

.. Save the following YAML in the `nro-deployment.yaml` file:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: numa-deployment-1
  namespace: openshift-numaresources
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      schedulerName: topo-aware-scheduler <1>
      containers:
      - name: ctnr
        image: quay.io/openshifttest/hello-openshift:openshift
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: "100Mi"
            cpu: "10"
          requests:
            memory: "100Mi"
            cpu: "10"
      - name: ctnr2
        image: registry.access.redhat.com/rhel:latest
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "-c"]
        args: [ "while true; do sleep 1h; done;" ]
        resources:
          limits:
            memory: "100Mi"
            cpu: "8"
          requests:
            memory: "100Mi"
            cpu: "8"
----
<1> `schedulerName` must match the name of the NUMA-aware scheduler that is deployed in your cluster, for example `topo-aware-scheduler`.

.. Create the `Deployment` CR by running the following command:
+
[source,terminal]
----
$ oc create -f nro-deployment.yaml
----

.Verification

. Verify that the deployment was successful:
+
[source,terminal]
----
$ oc get pods -n openshift-numaresources
----
+
.Example output
[source,terminal]
----
NAME                                                READY   STATUS    RESTARTS   AGE
numa-deployment-1-6c4f5bdb84-wgn6g                  2/2     Running   0          5m2s
numaresources-controller-manager-7d9d84c58d-4v65j   1/1     Running   0          18m
numaresourcesoperator-worker-7d96r                  2/2     Running   4          43m
numaresourcesoperator-worker-crsht                  2/2     Running   2          43m
numaresourcesoperator-worker-jp9mw                  2/2     Running   2          43m
secondary-scheduler-847cb74f84-fpncj                1/1     Running   0          18m
----

. Verify that the `topo-aware-scheduler` is scheduling the deployed pod by running the following command:
+
[source,terminal]
----
$ oc describe pod numa-deployment-1-6c4f5bdb84-wgn6g -n openshift-numaresources
----
+
.Example output
[source,terminal]
----
Events:
  Type    Reason          Age    From                  Message
  ----    ------          ----   ----                  -------
  Normal  Scheduled       4m45s  topo-aware-scheduler  Successfully assigned openshift-numaresources/numa-deployment-1-6c4f5bdb84-wgn6g to worker-1
----
+
[NOTE]
====
Deployments that request more resources than is available for scheduling will fail with a `MinimumReplicasUnavailable` error. The deployment succeeds when the required resources become available. Pods remain in the `Pending` state until the required resources are available.
====

. Verify that the expected allocated resources are listed for the node.

.. Identify the node that is running the deployment pod by running the following command:
+
[source,terminal]
----
$ oc get pods -n openshift-numaresources -o wide
----
+
.Example output
[source,terminal]
----
NAME                                 READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
numa-deployment-1-6c4f5bdb84-wgn6g   0/2     Running   0          82m   10.128.2.50   worker-1   <none>  <none>
----
+
.. Run the following command with the name of that node that is running the deployment pod.
+
[source,terminal]
----
$ oc describe noderesourcetopologies.topology.node.k8s.io worker-1
----
+
.Example output
[source,terminal]
----
...

Zones:
  Costs:
    Name:   node-0
    Value:  10
    Name:   node-1
    Value:  21
  Name:     node-0
  Resources:
    Allocatable:  39
    Available:    21 <1>
    Capacity:     40
    Name:         cpu
    Allocatable:  6442450944
    Available:    6442450944
    Capacity:     6442450944
    Name:         hugepages-1Gi
    Allocatable:  134217728
    Available:    134217728
    Capacity:     134217728
    Name:         hugepages-2Mi
    Allocatable:  262415904768
    Available:    262206189568
    Capacity:     270146007040
    Name:         memory
  Type:           Node
----
<1> The `Available` capacity is reduced because of the resources that have been allocated to the guaranteed pod.
+
Resources consumed by guaranteed pods are subtracted from the available node resources listed under `noderesourcetopologies.topology.node.k8s.io`.

. Resource allocations for pods with a `Best-effort` or `Burstable` quality of service (`qosClass`) are not reflected in the NUMA node resources under `noderesourcetopologies.topology.node.k8s.io`. If a pod's consumed resources are not reflected in the node resource calculation, verify that the pod has `qosClass` of `Guaranteed` and the CPU request is an integer value, not a decimal value. You can verify the that the pod has a  `qosClass` of `Guaranteed` by running the following command:
+
[source,terminal]
----
$ oc get pod numa-deployment-1-6c4f5bdb84-wgn6g -n openshift-numaresources -o jsonpath="{ .status.qosClass }"
----
+
.Example output
[source,terminal]
----
Guaranteed
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_mod-docs-content-type: PROCEDURE

[id="cnf-configuring-node-groups-for-the-numaresourcesoperator_{context}"]
= Optional: Configuring polling operations for NUMA resources updates

The daemons controlled by the NUMA Resources Operator in their `nodeGroup` poll resources to retrieve updates about available NUMA resources. You can fine-tune polling operations for these daemons by configuring the `spec.nodeGroups` specification in the `NUMAResourcesOperator` custom resource (CR). This provides advanced control of polling operations. Configure these specifications to improve scheduling behavior and troubleshoot suboptimal scheduling decisions.

The configuration options are the following:

* `infoRefreshMode`: Determines the trigger condition for polling the kubelet. The NUMA Resources Operator reports the resulting information to the API server.
* `infoRefreshPeriod`: Determines the duration between polling updates.
* `podsFingerprinting`: Determines if point-in-time information for the current set of pods running on a node is exposed in polling updates.
+
[NOTE]
====
The default value for `podsFingerprinting` is `EnabledExclusiveResources`. To optimize scheduler performance, set `podsFingerprinting` to either `EnabledExclusiveResources` or `Enabled`. Additionally, configure the `cacheResyncPeriod` in the `NUMAResourcesScheduler` custom resource (CR) to a value greater than 0. The `cacheResyncPeriod` specification helps to report more exact resource availability by monitoring pending resources on nodes.
====

.Prerequisites

* Install the OpenShift CLI (`oc`).
* Log in as a user with `cluster-admin` privileges.
* Install the NUMA Resources Operator.

.Procedure

* Configure the `spec.nodeGroups` specification in your `NUMAResourcesOperator` CR:
+
[source,yaml]
----
apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesOperator
metadata:
  name: numaresourcesoperator
spec:
  nodeGroups:
  - config:
      infoRefreshMode: Periodic <1>
      infoRefreshPeriod: 10s <2>
      podsFingerprinting: Enabled <3>
    name: worker
----
<1> Valid values are `Periodic`, `Events`, `PeriodicAndEvents`. Use `Periodic` to poll the kubelet at intervals that you define in `infoRefreshPeriod`. Use `Events` to poll the kubelet at every pod lifecycle event. Use `PeriodicAndEvents` to enable both methods.
<2> Define the polling interval for `Periodic` or `PeriodicAndEvents` refresh modes. The field is ignored if the refresh mode is `Events`.
<3> Valid values are `Enabled`, `Disabled`, and `EnabledExclusiveResources`. Setting to `Enabled` or `EnabledExclusiveResources` is a requirement for the `cacheResyncPeriod` specification in the `NUMAResourcesScheduler`.

.Verification

. After you deploy the NUMA Resources Operator, verify that the node group configurations were applied by running the following command:
+
[source,terminal]
----
$ oc get numaresop numaresourcesoperator -o json | jq '.status'
----
+
.Example output
[source,terminal]
----
      ...

        "config": {
        "infoRefreshMode": "Periodic",
        "infoRefreshPeriod": "10s",
        "podsFingerprinting": "Enabled"
      },
      "name": "worker"

      ...
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_mod-docs-content-type: PROCEDURE
[id="cnf-troubleshooting-numa-aware-workloads_{context}"]
= Troubleshooting NUMA-aware scheduling

To troubleshoot common problems with NUMA-aware pod scheduling, perform the following steps.

.Prerequisites

* Install the {product-title} CLI (`oc`).

* Log in as a user with cluster-admin privileges.

* Install the NUMA Resources Operator and deploy the NUMA-aware secondary scheduler.

.Procedure

. Verify that the `noderesourcetopologies` CRD is deployed in the cluster by running the following command:
+
[source,terminal]
----
$ oc get crd | grep noderesourcetopologies
----
+
.Example output
[source,terminal]
----
NAME                                                              CREATED AT
noderesourcetopologies.topology.node.k8s.io                       2022-01-18T08:28:06Z
----

. Check that the NUMA-aware scheduler name matches the name specified in your NUMA-aware workloads by running the following command:
+
[source,terminal]
----
$ oc get numaresourcesschedulers.nodetopology.openshift.io numaresourcesscheduler -o json | jq '.status.schedulerName'
----
+
.Example output
[source,terminal]
----
topo-aware-scheduler
----

. Verify that NUMA-aware schedulable nodes have the `noderesourcetopologies` CR applied to them. Run the following command:
+
[source,terminal]
----
$ oc get noderesourcetopologies.topology.node.k8s.io
----
+
.Example output
[source,terminal]
----
NAME                    AGE
compute-0.example.com   17h
compute-1.example.com   17h
----
+
[NOTE]
====
The number of nodes should equal the number of worker nodes that are configured by the machine config pool (`mcp`) worker definition.
====

. Verify the NUMA zone granularity for all schedulable nodes by running the following command:
+
[source,terminal]
----
$ oc get noderesourcetopologies.topology.node.k8s.io -o yaml
----
+
.Example output
[source,yaml]
----
apiVersion: v1
items:
- apiVersion: topology.node.k8s.io/v1
  kind: NodeResourceTopology
  metadata:
    annotations:
      k8stopoawareschedwg/rte-update: periodic
    creationTimestamp: "2022-06-16T08:55:38Z"
    generation: 63760
    name: worker-0
    resourceVersion: "8450223"
    uid: 8b77be46-08c0-4074-927b-d49361471590
  topologyPolicies:
  - SingleNUMANodeContainerLevel
  zones:
  - costs:
    - name: node-0
      value: 10
    - name: node-1
      value: 21
    name: node-0
    resources:
    - allocatable: "38"
      available: "38"
      capacity: "40"
      name: cpu
    - allocatable: "134217728"
      available: "134217728"
      capacity: "134217728"
      name: hugepages-2Mi
    - allocatable: "262352048128"
      available: "262352048128"
      capacity: "270107316224"
      name: memory
    - allocatable: "6442450944"
      available: "6442450944"
      capacity: "6442450944"
      name: hugepages-1Gi
    type: Node
  - costs:
    - name: node-0
      value: 21
    - name: node-1
      value: 10
    name: node-1
    resources:
    - allocatable: "268435456"
      available: "268435456"
      capacity: "268435456"
      name: hugepages-2Mi
    - allocatable: "269231067136"
      available: "269231067136"
      capacity: "270573244416"
      name: memory
    - allocatable: "40"
      available: "40"
      capacity: "40"
      name: cpu
    - allocatable: "1073741824"
      available: "1073741824"
      capacity: "1073741824"
      name: hugepages-1Gi
    type: Node
- apiVersion: topology.node.k8s.io/v1
  kind: NodeResourceTopology
  metadata:
    annotations:
      k8stopoawareschedwg/rte-update: periodic
    creationTimestamp: "2022-06-16T08:55:37Z"
    generation: 62061
    name: worker-1
    resourceVersion: "8450129"
    uid: e8659390-6f8d-4e67-9a51-1ea34bba1cc3
  topologyPolicies:
  - SingleNUMANodeContainerLevel
  zones: <1>
  - costs:
    - name: node-0
      value: 10
    - name: node-1
      value: 21
    name: node-0
    resources: <2>
    - allocatable: "38"
      available: "38"
      capacity: "40"
      name: cpu
    - allocatable: "6442450944"
      available: "6442450944"
      capacity: "6442450944"
      name: hugepages-1Gi
    - allocatable: "134217728"
      available: "134217728"
      capacity: "134217728"
      name: hugepages-2Mi
    - allocatable: "262391033856"
      available: "262391033856"
      capacity: "270146301952"
      name: memory
    type: Node
  - costs:
    - name: node-0
      value: 21
    - name: node-1
      value: 10
    name: node-1
    resources:
    - allocatable: "40"
      available: "40"
      capacity: "40"
      name: cpu
    - allocatable: "1073741824"
      available: "1073741824"
      capacity: "1073741824"
      name: hugepages-1Gi
    - allocatable: "268435456"
      available: "268435456"
      capacity: "268435456"
      name: hugepages-2Mi
    - allocatable: "269192085504"
      available: "269192085504"
      capacity: "270534262784"
      name: memory
    type: Node
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----
<1> Each stanza under `zones` describes the resources for a single NUMA zone.
<2> `resources` describes the current state of the NUMA zone resources. Check that resources listed under `items.zones.resources.available` correspond to the exclusive NUMA zone resources allocated to each guaranteed pod.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_module-type: PROCEDURE
[id="cnf-reporting-more-exact-resource-availability_{context}"]
= Reporting more exact resource availability

Enable the `cacheResyncPeriod` specification to help the NUMA Resources Operator report more exact resource availability by monitoring pending resources on nodes and synchronizing this information in the scheduler cache at a defined interval. This also helps to minimize Topology Affinity Error errors because of sub-optimal scheduling decisions. The lower the interval, the greater the network load. The `cacheResyncPeriod` specification is disabled by default.

.Prerequisites

* Install the OpenShift CLI (`oc`).
* Log in as a user with `cluster-admin` privileges.

.Procedure

. Delete the currently running `NUMAResourcesScheduler` resource:

.. Get the active `NUMAResourcesScheduler` by running the following command:
+
[source,terminal]
----
$ oc get NUMAResourcesScheduler
----
+
.Example output
[source,terminal]
----
NAME                     AGE
numaresourcesscheduler   92m
----

.. Delete the secondary scheduler resource by running the following command:
+
[source,terminal]
----
$ oc delete NUMAResourcesScheduler numaresourcesscheduler
----
+
.Example output
[source,terminal]
----
numaresourcesscheduler.nodetopology.openshift.io "numaresourcesscheduler" deleted
----

. Save the following YAML in the file `nro-scheduler-cacheresync.yaml`. This example changes the log level to `Debug`:
+
[source,yaml,subs="attributes+"]
----
apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-container-rhel8:v{product-version}"
  cacheResyncPeriod: "5s" <1>
----
+
<1> Enter an interval value in seconds for synchronization of the scheduler cache. A value of `5s` is typical for most implementations.

. Create the updated `NUMAResourcesScheduler` resource by running the following command:
+
[source,terminal]
----
$ oc create -f nro-scheduler-cacheresync.yaml
----
+
.Example output
[source,terminal]
----
numaresourcesscheduler.nodetopology.openshift.io/numaresourcesscheduler created
----

.Verification steps

. Check that the NUMA-aware scheduler was successfully deployed:

.. Run the following command to check that the CRD is created successfully:
+
[source,terminal]
----
$ oc get crd | grep numaresourcesschedulers
----
+
.Example output
[source,terminal]
----
NAME                                                              CREATED AT
numaresourcesschedulers.nodetopology.openshift.io                 2022-02-25T11:57:03Z
----

.. Check that the new custom scheduler is available by running the following command:
+
[source,terminal]
----
$ oc get numaresourcesschedulers.nodetopology.openshift.io
----
+
.Example output
[source,terminal]
----
NAME                     AGE
numaresourcesscheduler   3h26m
----

. Check that the logs for the scheduler show the increased log level:

.. Get the list of pods running in the `openshift-numaresources` namespace by running the following command:
+
[source,terminal]
----
$ oc get pods -n openshift-numaresources
----
+
.Example output
[source,terminal]
----
NAME                                               READY   STATUS    RESTARTS   AGE
numaresources-controller-manager-d87d79587-76mrm   1/1     Running   0          46h
numaresourcesoperator-worker-5wm2k                 2/2     Running   0          45h
numaresourcesoperator-worker-pb75c                 2/2     Running   0          45h
secondary-scheduler-7976c4d466-qm4sc               1/1     Running   0          21m
----

.. Get the logs for the secondary scheduler pod by running the following command:
+
[source,terminal]
----
$ oc logs secondary-scheduler-7976c4d466-qm4sc -n openshift-numaresources
----
+
.Example output
[source,terminal]
----
...
I0223 11:04:55.614788       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.Namespace total 11 items received
I0223 11:04:56.609114       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.ReplicationController total 10 items received
I0223 11:05:22.626818       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.StorageClass total 7 items received
I0223 11:05:31.610356       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.PodDisruptionBudget total 7 items received
I0223 11:05:31.713032       1 eventhandlers.go:186] "Add event for scheduled pod" pod="openshift-marketplace/certified-operators-thtvq"
I0223 11:05:53.461016       1 eventhandlers.go:244] "Delete event for scheduled pod" pod="openshift-marketplace/certified-operators-thtvq"
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_mod-docs-content-type: PROCEDURE
[id="cnf-changing-where-high-performance-workloads-run_{context}"]
= Changing where high-performance workloads run

The NUMA-aware secondary scheduler is responsible for scheduling high-performance workloads on a worker node and within a NUMA node where the workloads can be optimally processed. By default, the secondary scheduler assigns workloads to the NUMA node within the chosen worker node that has the most available resources.

If you want to change where the workloads run, you can add the `scoringStrategy` setting to the `NUMAResourcesScheduler` custom resource and set its value to either `MostAllocated`  or `BalancedAllocation`.

.Prerequisites

* Install the OpenShift CLI (`oc`).
* Log in as a user with `cluster-admin` privileges.

.Procedure

. Delete the currently running `NUMAResourcesScheduler` resource by using the following steps:

.. Get the active `NUMAResourcesScheduler` by running the following command:
+
[source,terminal]
----
$ oc get NUMAResourcesScheduler
----
+
.Example output
[source,terminal]
----
NAME                     AGE
numaresourcesscheduler   92m
----

.. Delete the secondary scheduler resource by running the following command:
+
[source,terminal]
----
$ oc delete NUMAResourcesScheduler numaresourcesscheduler
----
+
.Example output
[source,terminal]
----
numaresourcesscheduler.nodetopology.openshift.io "numaresourcesscheduler" deleted
----

. Save the following YAML in the file `nro-scheduler-mostallocated.yaml`. This example changes the `scoringStrategy` to `MostAllocated`:
+
[source,yaml]
----
apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-container-rhel8:v{product-version}"
  scoringStrategy:
        type: "MostAllocated" <1>
----
<1> If the `scoringStrategy` configuration is omitted, the default of `LeastAllocated` applies.

. Create the updated `NUMAResourcesScheduler` resource by running the following command:
+
[source,terminal]
----
$ oc create -f nro-scheduler-mostallocated.yaml
----
+
.Example output
[source,terminal]
----
numaresourcesscheduler.nodetopology.openshift.io/numaresourcesscheduler created
----

.Verification

. Check that the NUMA-aware scheduler was successfully deployed by using the following steps:

.. Run the following command to check that the custom resource definition (CRD) is created successfully:
+
[source,terminal]
----
$ oc get crd | grep numaresourcesschedulers
----
+
.Example output
[source,terminal]
----
NAME                                                              CREATED AT
numaresourcesschedulers.nodetopology.openshift.io                 2022-02-25T11:57:03Z
----

.. Check that the new custom scheduler is available by running the following command:
+
[source,terminal]
----
$ oc get numaresourcesschedulers.nodetopology.openshift.io
----
+
.Example output
[source,terminal]
----
NAME                     AGE
numaresourcesscheduler   3h26m
----

. Verify that the `ScoringStrategy` has been applied correctly by running the following command to check the relevant `ConfigMap` resource for the scheduler:
+
[source,terminal]
----
$ oc get -n openshift-numaresources cm topo-aware-scheduler-config -o yaml | grep scoring -A 1
----
+
.Example output
[source,terminal]
----
scoringStrategy:
  type: MostAllocated
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_module-type: PROCEDURE
[id="cnf-checking-numa-aware-scheduler-logs_{context}"]
= Checking the NUMA-aware scheduler logs

Troubleshoot problems with the NUMA-aware scheduler by reviewing the logs. If required, you can increase the scheduler log level by modifying the `spec.logLevel` field of the `NUMAResourcesScheduler` resource. Acceptable values are `Normal`, `Debug`, and `Trace`, with `Trace` being the most verbose option.

[NOTE]
====
To change the log level of the secondary scheduler, delete the running scheduler resource and re-deploy it with the changed log level. The scheduler is unavailable for scheduling new workloads during this downtime.
====

.Prerequisites

* Install the OpenShift CLI (`oc`).
* Log in as a user with `cluster-admin` privileges.

.Procedure

. Delete the currently running `NUMAResourcesScheduler` resource:

.. Get the active `NUMAResourcesScheduler` by running the following command:
+
[source,terminal]
----
$ oc get NUMAResourcesScheduler
----
+
.Example output
[source,terminal]
----
NAME                     AGE
numaresourcesscheduler   90m
----

.. Delete the secondary scheduler resource by running the following command:
+
[source,terminal]
----
$ oc delete NUMAResourcesScheduler numaresourcesscheduler
----
+
.Example output
[source,terminal]
----
numaresourcesscheduler.nodetopology.openshift.io "numaresourcesscheduler" deleted
----

. Save the following YAML in the file `nro-scheduler-debug.yaml`. This example changes the log level to `Debug`:
+
[source,yaml,subs="attributes+"]
----
apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-container-rhel8:v{product-version}"
  logLevel: Debug
----

. Create the updated `Debug` logging `NUMAResourcesScheduler` resource by running the following command:
+
[source,terminal]
----
$ oc create -f nro-scheduler-debug.yaml
----
+
.Example output
[source,terminal]
----
numaresourcesscheduler.nodetopology.openshift.io/numaresourcesscheduler created
----

.Verification steps

. Check that the NUMA-aware scheduler was successfully deployed:

.. Run the following command to check that the CRD is created successfully:
+
[source,terminal]
----
$ oc get crd | grep numaresourcesschedulers
----
+
.Example output
[source,terminal]
----
NAME                                                              CREATED AT
numaresourcesschedulers.nodetopology.openshift.io                 2022-02-25T11:57:03Z
----

.. Check that the new custom scheduler is available by running the following command:
+
[source,terminal]
----
$ oc get numaresourcesschedulers.nodetopology.openshift.io
----
+
.Example output
[source,terminal]
----
NAME                     AGE
numaresourcesscheduler   3h26m
----

. Check that the logs for the scheduler shows the increased log level:

.. Get the list of pods running in the `openshift-numaresources` namespace by running the following command:
+
[source,terminal]
----
$ oc get pods -n openshift-numaresources
----
+
.Example output
[source,terminal]
----
NAME                                               READY   STATUS    RESTARTS   AGE
numaresources-controller-manager-d87d79587-76mrm   1/1     Running   0          46h
numaresourcesoperator-worker-5wm2k                 2/2     Running   0          45h
numaresourcesoperator-worker-pb75c                 2/2     Running   0          45h
secondary-scheduler-7976c4d466-qm4sc               1/1     Running   0          21m
----

.. Get the logs for the secondary scheduler pod by running the following command:
+
[source,terminal]
----
$ oc logs secondary-scheduler-7976c4d466-qm4sc -n openshift-numaresources
----
+
.Example output
[source,terminal]
----
...
I0223 11:04:55.614788       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.Namespace total 11 items received
I0223 11:04:56.609114       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.ReplicationController total 10 items received
I0223 11:05:22.626818       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.StorageClass total 7 items received
I0223 11:05:31.610356       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.PodDisruptionBudget total 7 items received
I0223 11:05:31.713032       1 eventhandlers.go:186] "Add event for scheduled pod" pod="openshift-marketplace/certified-operators-thtvq"
I0223 11:05:53.461016       1 eventhandlers.go:244] "Delete event for scheduled pod" pod="openshift-marketplace/certified-operators-thtvq"
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_module-type: PROCEDURE
[id="cnf-troubleshooting-resource-topo-exporter_{context}"]
= Troubleshooting the resource topology exporter

Troubleshoot `noderesourcetopologies` objects where unexpected results are occurring by inspecting the corresponding `resource-topology-exporter` logs.

[NOTE]
====
It is recommended that NUMA resource topology exporter instances in the cluster are named for nodes they refer to. For example, a worker node with the name `worker` should have a corresponding `noderesourcetopologies` object called `worker`.
====

.Prerequisites

* Install the OpenShift CLI (`oc`).
* Log in as a user with `cluster-admin` privileges.

.Procedure

. Get the daemonsets managed by the NUMA Resources Operator. Each daemonset has a corresponding `nodeGroup` in the `NUMAResourcesOperator` CR. Run the following command:
+
[source,terminal]
----
$ oc get numaresourcesoperators.nodetopology.openshift.io numaresourcesoperator -o jsonpath="{.status.daemonsets[0]}"
----
+
.Example output
[source,json]
----
{"name":"numaresourcesoperator-worker","namespace":"openshift-numaresources"}
----

. Get the label for the daemonset of interest using the value for `name` from the previous step:
+
[source,terminal]
----
$ oc get ds -n openshift-numaresources numaresourcesoperator-worker -o jsonpath="{.spec.selector.matchLabels}"
----
+
.Example output
[source,json]
----
{"name":"resource-topology"}
----

. Get the pods using the `resource-topology` label by running the following command:
+
[source,terminal]
----
$ oc get pods -n openshift-numaresources -l name=resource-topology -o wide
----
+
.Example output
[source,terminal]
----
NAME                                 READY   STATUS    RESTARTS   AGE    IP            NODE
numaresourcesoperator-worker-5wm2k   2/2     Running   0          2d1h   10.135.0.64   compute-0.example.com
numaresourcesoperator-worker-pb75c   2/2     Running   0          2d1h   10.132.2.33   compute-1.example.com
----

. Examine the logs of the `resource-topology-exporter` container running on the worker pod that corresponds to the node you are troubleshooting. Run the following command:
+
[source,terminal]
----
$ oc logs -n openshift-numaresources -c resource-topology-exporter numaresourcesoperator-worker-pb75c
----
+
.Example output
[source,terminal]
----
I0221 13:38:18.334140       1 main.go:206] using sysinfo:
reservedCpus: 0,1
reservedMemory:
  "0": 1178599424
I0221 13:38:18.334370       1 main.go:67] === System information ===
I0221 13:38:18.334381       1 sysinfo.go:231] cpus: reserved "0-1"
I0221 13:38:18.334493       1 sysinfo.go:237] cpus: online "0-103"
I0221 13:38:18.546750       1 main.go:72]
cpus: allocatable "2-103"
hugepages-1Gi:
  numa cell 0 -> 6
  numa cell 1 -> 1
hugepages-2Mi:
  numa cell 0 -> 64
  numa cell 1 -> 128
memory:
  numa cell 0 -> 45758Mi
  numa cell 1 -> 48372Mi
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// *scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_module-type: PROCEDURE
[id="cnf-troubleshooting-missing-rte-config-maps_{context}"]
= Correcting a missing resource topology exporter config map

If you install the NUMA Resources Operator in a cluster with misconfigured cluster settings, in some circumstances, the Operator is shown as active but the logs of the resource topology exporter (RTE) daemon set pods show that the configuration for the RTE is missing, for example:

[source,text]
----
Info: couldn't find configuration in "/etc/resource-topology-exporter/config.yaml"
----

This log message indicates that the `kubeletconfig` with the required configuration was not properly applied in the cluster, resulting in a missing RTE `configmap`. For example, the following cluster is missing a `numaresourcesoperator-worker` `configmap` custom resource (CR):

[source,terminal]
----
$ oc get configmap
----

.Example output
[source,terminal]
----
NAME                           DATA   AGE
0e2a6bd3.openshift-kni.io      0      6d21h
kube-root-ca.crt               1      6d21h
openshift-service-ca.crt       1      6d21h
topo-aware-scheduler-config    1      6d18h
----

In a correctly configured cluster, `oc get configmap` also returns a `numaresourcesoperator-worker` `configmap` CR.

.Prerequisites

* Install the {product-title} CLI (`oc`).

* Log in as a user with cluster-admin privileges.

* Install the NUMA Resources Operator and deploy the NUMA-aware secondary scheduler.

.Procedure

. Compare the values for `spec.machineConfigPoolSelector.matchLabels` in `kubeletconfig` and
`metadata.labels` in the `MachineConfigPool` (`mcp`) worker CR using the following commands:

..  Check the `kubeletconfig` labels by running the following command:
+
[source,terminal]
----
$ oc get kubeletconfig -o yaml
----
+
.Example output
[source,yaml]
----
machineConfigPoolSelector:
  matchLabels:
    cnf-worker-tuning: enabled
----

.. Check the `mcp` labels by running the following command:
+
[source,terminal]
----
$ oc get mcp worker -o yaml
----
+
.Example output
[source,yaml]
----
labels:
  machineconfiguration.openshift.io/mco-built-in: ""
  pools.operator.machineconfiguration.openshift.io/worker: ""
----
+
The `cnf-worker-tuning: enabled` label is not present in the `MachineConfigPool` object.

. Edit the `MachineConfigPool` CR to include the missing label, for example:
+
[source,terminal]
----
$ oc edit mcp worker -o yaml
----
+
.Example output
[source,yaml]
----
labels:
  machineconfiguration.openshift.io/mco-built-in: ""
  pools.operator.machineconfiguration.openshift.io/worker: ""
  cnf-worker-tuning: enabled
----

. Apply the label changes and wait for the cluster to apply the updated configuration. Run the following command:

.Verification

* Check that the missing `numaresourcesoperator-worker` `configmap` CR is applied:
+
[source,terminal]
----
$ oc get configmap
----
+
.Example output
[source,terminal]
----
NAME                           DATA   AGE
0e2a6bd3.openshift-kni.io      0      6d21h
kube-root-ca.crt               1      6d21h
numaresourcesoperator-worker   1      5m
openshift-service-ca.crt       1      6d21h
topo-aware-scheduler-config    1      6d18h
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * scalability_and_performance/cnf-numa-aware-scheduling.adoc

:_mod-docs-content-type: PROCEDURE
[id="cnf-about-collecting-nro-data_{context}"]
= Collecting NUMA Resources Operator data

You can use the `oc adm must-gather` CLI command to collect information about your cluster, including features and objects associated with the NUMA Resources Operator.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.

* You have installed the {oc-first}.

.Procedure

* To collect NUMA Resources Operator data with `must-gather`, you must specify the NUMA Resources Operator `must-gather` image.
+
[source,terminal,subs="attributes+"]
----
$ oc adm must-gather --image=registry.redhat.io/openshift4/numaresources-must-gather-rhel9:v{product-version}
----

:leveloffset!:
